% !TEX encoding = UTF-8 Unicode
% !TEX root = tese.tex

\cite

\chapter{Experimentos}
\label{ch:tony}


\section{Primeiros Experimentos}
Partindo desta pesquisa inicial, procurei desenvolver alguns experimentos musicais iniciais.  Por um lado, tinham as tecnologias de programação, que estavam surgindo naquele momento, mas também havia um repertório e teorias da música que se colocavam com meu ingresso no Doutorado. Esses primeiros estudos que eram projetos bastante simples, foram importantes como exercícios estéticos e para começar um aprendizado dessas novas tecnologias.


\subsection{Bandas Críticas}
Bandas Críticas pode ser considerada a primeira ``peça'' musical que escrevi. Até então, fazia apenas ``música''. Foi um primeiro exercício de escrita de uma partitura gráfica e o primeiro exercício de composição para uma equipe tocar. A peça foi composta a partir da proposta apresentada pelo Estúdio Fita Crepe \footnote{O Estúdio Fita Crepe foi um espaço dedicado à produção e performance de música experimental na cidade de São Paulo. \url{http://www.estudiofitacrepesp.com/}} de através do músico Ricardo Garcia ao NuSom, de um concerto de ``Música Silenciosa'', que seria uma ``música para acariciar os ouvidos'' que se embasaria em sutilezas de timbres e volumes.

A proposta foi de explorar metalinguisticamente o conceito de bandas críticas da audição, como apontamos em uma proposta de apresentação para o Simpósio Brasileiro de Computação Musical de 2015, que ocorreu em Campinas:


\begin{citacao}
A peça explora as frequências centrais e limítrofes das bandas críticas da audição, sobrepondo harmônicos e criando dissonâncias a partir de sons fora das escalas musicais tradicionais. Surge de um processo de composição metalinguístico, de investigar a própria percepção acústica, em um desejo de materialização de conceitos psicoacústicos em objetos sonoros. 
Surge de um processo de composição metalinguístico, de investigar a própria percepção acústica, em um desejo de materialização dos conceitos psicoacústicos em objetos sonoros. No caso, representa uma materialização do conceito de bandas críticas, buscando explorar sensações audíveis e artefatos emergentes da exploração desse conceito. A performance é programada para 3 músicos, cada um conectado a uma caixa de som diferente, para enfatizar as diferenças entre as frequências exploradas. \cite{ArianeStolfi2015}
\end{citacao}

Também foi uma experiência de composição abstrata, a partida da idéia de buscar uma saturação sensória formada a partir de frequências puras mas não necessariamente harmônicas, que fosse estimular todas as regiões da cóclea. A peça é estruturada em quatro partes, um primeiro movimento introdutório onde os músicos não performam e o processamento acontece de forma automatizada por um \emph{patch} de PD (Figura \ref{bandaspatch}), com todos os 44 osciladores ligando em um volume ascendente; um segundo momento, os músicos vão desligando os osciladores de acordo com uma ordem pré-estabelecidas, causando uma rarefação, até restarem apenas poucos osciladores ligados; na terceira parte os músicos improvisam a partir de harmônicos desses osciladores ligados; por fim, há um momento de ``destruição da onda'', onde os músicos passam a improvisar desenhando a onda, o que gera uma harmonia de ruídos, até a destruição completa dessas harmonias com a busca do silêncio.

\begin{figure}[htb]
    \caption{\label{bandaspartitura}Partitura gráfica para a peça Bandas Criticas. }
    \begin{center}
    \includegraphics[width=1\linewidth]{pictures/cap3/bandascriticaspartitura}
    \end{center}
    \legend{Fonte: Screenshot da autora, Janeiro de 2016}
\end{figure}

Fizemos uma apresentação no Estúdio Fita Crepe, no evento ``Música? 11'', onde performaram junto comigo Davi Donato e Sérgio Abdalla, no dia 29 de agosto de 2015 e outra no Simpósio Brasileiro de Computação Musical no dia 24 de novembro em Campinas, com a participação de Davi Donato e Luzilei Aliel. Análise do registro das gravações mostraram que o espectrograma da peça ficou bem próximo ao desenho da partitura pensado originalmente, como podemos ver na imagem \ref{bandasciticasspec}. 

\begin{figure}[htb]
    \caption{\label{bandaspatch}Patch para a peça Bandas Criticas. }
    \begin{center}
    \includegraphics[width=1\linewidth]{pictures/cap3/bandascriticaspd}
    \end{center}
    \legend{Fonte: Screenshot da autora, Janeiro de 2016}
\end{figure}

\begin{figure}[htb]
    \caption{\label{bandasciticasspec}Espectrogramas gerados a partir das gravações da peça no Estúdio Fita Crepe (acima) e no SBCM (abaixo). }
    \begin{center}
    \includegraphics[width=1\linewidth]{pictures/cap3/bandascriticasspectro2}
    \end{center}
    \legend{Fonte: Screenshot da autora, Janeiro de 2016}
\end{figure}



\subsection{QWERTY}
\label{sec:QWERTY}
Qwerty foi o primeiro experimento em web áudio desenvolvido após entrar no programa de doutorado. Como a idéia inicial do projeto era buscar formas de explorar os inputs dos computadores pessoais, partimos da principal forma de input que é o teclado QWERTY, que tem a disposição ainda das tradicionais máquinas de escrever. A idéia era transformar o teclado em uma máquina sonora, que não é em si uma coisa nova. O site Patatap, por exemplo, que mencionamos no capítulo anterior, é uma plataforma que funciona dessa maneira. Outro exemplo é o ``Typedrummer'' de Kyle Stetz \footnote{Disponível em: \url{http://typedrummer.com/}} , onde o usuário digita um texto, e para cada letra há um sample, e o texto é tocado em loop.


Para essa máquina, me inspirei nas leituras de Haroldo de Campos do seu poema épico Galáxias \cite{Campos2004}, que tem como estrutura um bloco contínuo de frases sinestésicas, sem qualquer tipo de pontuação ou espaçamento. 

Naquele momento, estava envolvida também no projeto de digitalização das revistas de poesia concreta Código \ref{codigo}, da revista de vanguarda lançada em 1973 e editada por Erthos Albino de Souza e Antônio Risério, que contém obras e traduções de vários poetas concretos desta geração. A revista Código, é como aponta Augusto de Campos ``acolheu materiais de vanguarda que não encontrariam guarida nas publicações convencionais alternativas" \cite{Scandurra2016}. Para a versão online da revista, para o qual programei a interface, seguindo uma idéia de design de navegação paratática, ou seja, que permitisse a re-combinação e re-montagem dos poemas de acordo com a vontade do leitor. Por conta deste projeto, estava em contato constante com as obras dos poetas desta geração, então partimos a pesquisa desta inspiração.

Parti do poema Galáxias porque ele é sobretudo extremamente sonoro. O livro acompanha um CD com as leituras de Haroldo de alguns fragmentos do seu texto, e foi de onde partimos para a extração dos primeiros samples, buscando uma otimização do discurso, em uma relação com a própria estrutura do poema, que como apontou o poeta Augusto de Campos em conversa com a autora, poderia ter tido sua primeira edição publicada em folhas soltas para ser recombinado, ideia que foi abandonada posteriormente.

\begin{figure}[htb]
    \caption{\label{codigo}Interface do site da digitalização da revista Código. }
    \begin{center}
    \includegraphics[width=1\linewidth]{pictures/cap3/codigo5.png}
    \end{center}
    \legend{Fonte: Screenshot da autora, Janeiro de 2016}
\end{figure}

Partimos de um processo de escuta buscado encontrar os sons correspondentes a cada letra em forma de token nas leituras do poeta e fizemos um mapa onde cada letra digitada do teclado corresponde a um fonema relacionado, criando um sampler de 26 botões, que são disparados ao digitar.

A interface desenvolvida foi a mais enxuta possível (figura \ref{qwerty}), pensando em um minimalismo radical que se relaciona com a estética da poesia concreta, e também em uma idéia de brutalismo digital que defendemos nesta pesquisa.

\begin{figure}[htb]
    \caption{\label{qwerty}Interface do experimento QWERTY, com fragmento de texto do Galáxias digitado. }
    \begin{center}
    \includegraphics[width=1\linewidth]{pictures/cap3/qwerty.png}
    \end{center}
    \legend{Fonte: Screenshot da autora, Janeiro de 2016}
\end{figure}





\subsection{Protesta Fora Temer}
Durante o mês de outubro de 2016, surgiu uma discussão na lista de emails da rede sonora sobre a possibilidade de se posicionar contra o impeachment da presidenta Dilma Rousseff, após uma breve discussão, verificou-se a dificuldade de elaborar um texto que fosse consenso entre as pessoas da rede, devido à ausência de um debate qualificado sobre o assunto e a divergência de opiniões políticas. 
Surgiu então a proposta de organizar um protesto sonoro, e foi lançado um pedido na lista, pela compositora Valéria Bonafé, para quem quisesse participar, que enviasse algum sample em mp3: 
\begin{citacao}
Conversei com algumas membras aqui da Sonora e propus que a gente organizasse um protesto-sonoro-fora-temer. Nossa ideia é montar um protesto sonoro colaborativo e interativo a partir de samples de áudio. O convite está aberto a todo mundo aqui da lista! Quem quiser participar basta enviar um (ou mais!) sample(s) de áudio. As indicações técnicas são apenas duas: que seja curto (questão de alguns segundos) e que esteja em MP3. A ideia é fazer algo leve (no sentido técnico) para não sobrecarregar o sistema de interação que vamos usar para articular esse banco de samples. Como relação ao conteúdo, a única coisa que combinamos é que o mot é “fora temer”. Mas também não precisa ser literal! Não é que precisa gravar apenas falando “fora temer”. Pode ser! Mas pode ser também algo em torno disso. Enfim, temos um tema. Mas cada um pode pensar algo a partir disso. O uso da criatividade, da imaginação, da exploração, da experimentação, da espontaneidade e tudo mais está em aberto! Assim como a “forma”. Pode ser uma gravação nua e crua, pode ser processada, por ser um recorte de algo etc. Enfim, a única diretriz de conteúdo é “fora temer” e a única diretriz técnica é “um sample curto e leve”.
\end{citacao}
Várias pessoas da lista mandaram contribuições e a partir do material enviado, e propus que fizéssemos, ao invés de uma faixa estática, um protesto sonoro interativo. Selecionei 25 samples de áudio e criei uma página html onde o movimento do mouse vai soltando aleatoriamente os samples cada vez que passa por cima de uma das palavras do site. O resultado pode ser uma massa de vozes – a maioria femininas – ou um coro protestante, dependendo da velocidade que o internauta percorre a página (figura \ref{protesta}). O site está disponível no endereço \url{http://www.sonora.me/protesta/foratemer/}. 

\begin{figure}[htb]
    \caption{\label{protesta}Protesta Sonora. }
    \begin{center}
    \includegraphics[width=1\linewidth]{pictures/banda_aberta_mob_crowd.png}
    \end{center}
    \legend{Fonte: Screenshot da autora dia 5 de janeiro de 2016}
\end{figure}



\section{Banda Aberta}
\begin{figure}[htb]
    \caption{\label{bandaabertamob}Banda aberta, imagem conceito para a proposta de intervenção pública}
    \begin{center}
    \includegraphics[width=1\linewidth]{pictures/banda_aberta_mob_crowd.png}
    \end{center}
    \legend{Fonte: desenho da autora}
\end{figure}


O projeto Banda Aberta começou a ser desenvolvido como parte da proposta do NuSom para o Festival Bigorna na Praça, organizado pelo Estúdio Fita Crepe na praça José Molina. A ideia inicial era conectar os dispositivos pessoais das pessoas pela internet, de modo a potencializar os amplificadores individuais dos celulares e pensar em uma proposta de performance participativa que envolvesse a audiência. Foi o primeiro projeto realizado em parceria com outro programador, o engenheiro de computação Fábio Goródscy, que estava realizando mestrado na Computação musical no IME e o primeiro grande projeto desenvolvido no âmbito desta pesquisa. A parceria com um cientista da computação permitiu que desenvolvêssemos uma estrutura mais complexa do que a dos primeiros experimentos, que ainda eram baseados principalmente em HTML5, e partir para a exploração de recursos mais complexos da Web Audio API \cite{Adenot2015}.

O principal objetivo desta performance era o dar voz ao coletivo, liberdade de fala, criando possibilidades de comunicação musical sem significado. Visamos permitir que pessoas em um espaço público venham a interagir em um contexto musical, fazendo um espetáculo produzido por elas mesmas, sem necessidade de nenhum conhecimento musical, utilizando seus próprios celulares, via wifi através de um sistema \emph{open-source} de chat sonoro. A figura abaixo apresenta uma representação da performance em um ambiente aberto. 

Performance com participação da audiência mediada por tecnologia é um tópico emergente no campo da tecnologia musical \cite{wu2017open} e na esfera da arte contemporânea. Em contraste com performances tradicionais, onde há uma divisão estrita entre audiência e performer, as performances participativas tendem a diluir os limites entre a audiência e o performer \cite{kattwinkel2003audience}. 

Na tradição artística brasileira, esse assunto tem sido explorado desde os anos 60, principalmente no trabalho artístico de neoconcretos como Hélio Oiticica e Lygia Clark. Os parangolés de Oiticica, desenvolvidos a partir de um trabalho do artista com a comunidade da mangueira no final dos anos 60 eram peças vestíveis, cujo sentido se dava na dança e na manipulação das formas pela comunidade, assim como os Bichos, de Lygia Clark, que são esculturas articuladas que podem ser manipuladas e re-compostas pelo público. \cite{Braga2008}

Atualmente, o desenvolvimento da tecnologia trouxe novos recursos para facilitar e encorajar processos participativos em performances e instalações como: dispositivos computacionais portáteis como Arduino\footnote{Plataforma eletrôncia em código aberto Arduino: \url{https://www.arduino.cc/}} ou Bela{Plataforma para processamento de áudio de baixa latência Bela: \url{http://bela.io/}}; smartphones; sensores de dimensão reduzida e novas linguagens versáteis de alto nível como Python e JavaScript, que também ajudaram a criar as bases para o campo de pesquisa da computação ubíqua. Diversos projetos utilizam tecnologias deste tipo para o emprego criativo da participação da audiência em performances ao vivo, como os projetos massMobile \cite{Weitzner2012}, Mood Conductor \cite{Fazekas:2014}, Open Symphony \cite{wu2017open}, Crowd in Cloud \cite{Lee2016} e TweetDreams \cite{Dahl2011}. Em condições ideais, os participante são capazes de interagir durante as performances interferindo na narrativa de modo sensível. 

O projeto de ambientes colaborativos para produção musical pode ser desafiador, no sentido de que a audiência não necessariamente compartilha de domínio de técnicas e processos musicais. O projeto Banda Aberta é uma tentativa de explorar esse tema, usando a música como ponto de partida, pela função social que ela tem de comunhão e comunicação \cite{Koelsch:2014}. A idéia era de propor uma obra ``aberta'', como defendida por Umberto Eco \cite{Eco1991}, que, de acordo com Robey \cite{Eco1991}, requer do público um grau muito maior de envolvimento pessoal e colaboração do que qualquer obra de arte tradicional do passado. Segundo Eco, em uma obra aberta, ``é decisão do artista de deixar com que parte da organização de seus constituintes seja relegada ao público ou ao acaso, dando a elas assim não uma ordenação definitiva, mas uma multiplicidade de ordens possíveis'' \cite{Eco1991}.

Com o projeto Banda Aberta, nós queríamos criar um sistema que pudesse ser facilmente utilizado como um instrumento musical pelos participantes, independente de qualquer conhecimento musical a priori. Existem vários tipos de interface para participação musical que partem de gestos de toque, simulando um processo de interação similar ao dos instrumentos tradicionais, como por exemplo a performance ``88 Fingers'' de Norbert Schnell e Benjamin Matuszewki \cite{Schnell2017}, onde cada participante pode controlar através do celular uma das teclas de um piano MIDI, ou ``Hyperconnected Action Painting", de Anna Xambó e Gerard Roma \cite{Xambo2017}, onde a audiência usa gestos com o celular para disparar sons pré definidos. Apesar de ambas serem divertidas como experiência participativa, o resultado sonoro nesses dois casos, na minha impressão pessoal, foi de uma massa de sons desordenada e fora de ritmo. Acredito que isso aconteceu em parte pela falta de prática em processos musicais coletivos por parte da audiência, que não necessariamente domina o gestual musical. Pensando nessa questão, queríamos propor uma ferramenta que pudesse ser acessada por qualquer usuário, independente de qualquer formação ou treinamento em práticas musicais, e para isso, procuramos partir de tecnologias que as pessoas já usam com mais desenvoltura.

\subsection{Descrição do Projeto}
Como já apontava McLuhan\cite{mcluhan1968comunicaccoes}, o alfabeto fonético é uma tecnologia fundamental para o desenvolvimento da cultura ocidental. Ele é fácil de se aprender e ajustável a várias linguagens, sendo assim, a base de toda cultura literata. Neste trabalho, nós fazemos uso do alfabeto como tecnologia para produzir música experimental. Texto e mensagens instantâneas se tornaram a principal forma de comunicação nos celulares hoje em dia, seja em comunicações síncronas ou assíncronas \cite{Madell:2007}, e por isso, decidimos experimentar com  o texto através de mensagens como forma de interação musical nesta primeira proposta. Muitos tipos de interfaces para produção musical usam o texto como forma de \emph{input}, como nos processos de \emph{live coding} \cite{Collins2003}, em processos participativos que usam feedback verbal da audiência \cite{noauthor_transglasphone_nodate}, em composições algorítmicas geradas por dados textuais ou sistemas que usam o teclado como controlador musical \cite{Fiebrink2007}. Neste projeto, nós procuramos criar um sistema que funcionasse para a produção musical de uma maneira análoga à relação entre o alfabeto e a linguagem oral.

A comunicação \emph{online} por texto se aproxima muito da comunicação oral, pela facilidade de escrita, e pela velocidade quase instantânea,\cite[33]{Levinson2001} Como aponta McLuhan, o conteúdo de um meio sempre agrega um meio anterior. O discurso, que é o meio mais antigo, está contido em quase todos meios subsequentes, como o livro, o telégrafo, o cinema e a televisão, e, é claro, a internet \cite[42]{Levinson2001}. 

McLuhan aponta que o espaço visual surge quando as consoantes foram criadas, ``como uma abstração sem significado", permitindo a análise das unidades silábicas em cada um de seus componentes. A consoante é para ele um não-som, algo que necessariamente ``soa junto". \cite[13-14]{mcluhan1968comunicaccoes} A base do discurso, segundo ele é o fonema, a unidade mínima do som, que seria a unidade mínima do som, na música, no entanto, Tenney  propõe em ``Meta-Hodos and Meta Hodos" o conceito de ``clang", que seria uma unidade \emph{gestalt} mínima reconhecível \cite[23]{Tenney1988}. 

Neste projeto, ao buscarmos o som das consoantes de um modo isolado, criamos uma desconstrução do discurso e procuramos reforçar esse caráter atomizante da linguagem escrita através de um duplo processo de conversão: discurso convertido em texto e posteriormente convertido em música.

A digitação é um processo muito mais simples para a maioria dos usuários de computadores e dispositivos digitais, em relação à operação de interfaces musicais, assim como a digitação na máquina de escrever era mais acessível para o homem comum do que o tocar de instrumentos analógicos. \cite[172]{Levinson2001}. Nossa ideia foi utilizar o \emph{input} de texto como forma de interação musical. 

Num projeto realizado previamente, o QWERTY, já tinha começado a explorar a digitação como forma de input, mapeando cada letra do teclado para um sample, que eram disparados de acordo com a digitação. Nós percebemos que isto exige do performer uma certa habilidade de controle do gesto, para que os sons saiam em um ritmo definido. Além disso, como as interfaces dos \emph{smartphones} não facilitam a digitação, queríamos pensar um sistema que não dependesse de agilidade para funcionar. Para encorajar a participação, nós projetamos um sistema que funcionava a partir de simples mensagens de texto.

No nosso projeto, utilizamos as frases como disparadores para uma sequência de samples, que são já compostos com durações fixas, num processo análogo ao de composição de frases musicais. Ao colocar um mecanismo de chat, as pessoas podem acessar o instrumento simultaneamente e participar de um diálogo sonoro. Conforme mais pessoas vão escrevendo simultaneamente, a camada de samples se torna mais densa e entrópica.

Diferente de processos de síntese vocal, nosso chat não concatena as sílabas, somente toca os sons pré-determinados em uma sequência, de modo que as palavras não são compreendidas como palavras, mas como frases musicais. Para adicionar um certo grau de controle à performance, utilizamos uma lógica da interface por linha de comando, através de certos comandos especiais que podem alterar a dinâmica da peça, ou alternar entre os conjuntos de samples compostos.

O projeto é construído a partir de dois dispositivos, o servidor de mensagens, que é programado em \emph{Ruby} e necessita de instalação em um servidor web, e a interface que faz o processamento das mensagens e é construída em HTML e \emph{JavaScript} e funciona a partir da Web Audio API. O sistema está disponível como código aberto\footnote{O código fonte está disponível em: \url{https://github.com/fabiogoro/bandaserver}}   

\begin{figure}[htb]
    \caption{\label{bandaabertaserver}Diagrama esquemático dos servidores do projeto Banda Aberta}
    \begin{center}
        \includegraphics[width=0.5\linewidth]{pictures/server.jpg}
    \end{center}
    \legend{Fonte: desenho da autora}
\end{figure}

Na primeira versão, o processamento de áudio funciona somente por sampleamento. Os samples são numerados de acordo com a tabela ASCII e estabeleci um sample para caractere da tabela. O sistema converte as letras nos números equivalentes segundo a tabela, e toca o sample respectivo segundo a ordem estabelecida por cada frase que é enviada ao chat. Posteriormente, desenvolvemos também uma segunda versão baseada somente em síntese que também funciona da mesma forma, com um mapeamento um-para-um, de um som para cada letra.

\subsection{Desenvolvimento do Projeto}
Para o desenvolvimento do projeto Banda Aberta, adotamos ``Lean UX'' como metodologia de design. Lean UX é uma metodologia ágil de desenvolvimento de software que tem sido aplicada em muitos projetos de webdesign \cite{leanux}. Envolve a criação de um protótipo mínimo e viável a partir de um estágio muito inicial do projeto. Este protótipo devem então ser testado em condições reais de uso e melhorias são feitas de modo iterativo, baseando-se em observações e feedbacks recolhidos. O projeto foi desenvolvido como software de código aberto e está disponível no Github. \footnote{Os códigos-fonte do projeto estão disponíveis em: \url{https://github.com/fabiogoro/bandaserver}}. A arquitetura do sistema foi descrita com mais detalhes no artigo publicado na conferência Audio Mostly de 2017 ``Open Band: A Platform for Collective Sound Dialogues''~\cite{Stolfi2017}.

Nosso primeiro protótipo contava apenas com o conjunto inicial de samples nomeado de ``Galáxias'', e pudemos testá-lo menos de um mês após a proposta inicial em uma reunião do NuSom. Os samples utilizados nessa primeira versão eram os mesmos reunidos para o experimento QWERTY, descrito na seção \ref{sec:QWERTY}. A experiência foi registrada e fizemos uma análise dos comentários dos participantes\footnote{Registro da experiência pode ser visto em: \url{https://www.youtube.com/watch?v=Utc_4mT5b8s}}. Pudemos notar desde o princípio que a performance tinha um certo grau de ludicidade, pelas reações de riso e divertimento dos presentes. 


Neste primeiro teste em público, notamos que haviam alguns problemas com relação à compatibilidade de aparelhos, que foram posteriormente corrigidos. Recebemos também a opinião de alguns usuários que notaram que após alguns minutos a experiência sonora passava a ser um pouco entediante, com a repetição constante dos mesmos sons. Propusemos então a criação de outros bancos de sons, que seriam alternados durante a performance, e que um deles fosse criado colaborativamente por um grupo de estudantes da Universidade Anhembi Morumbi, orientados pelo professor Vítor Kisil,  que também apresentariam uma peça no Festival Bigorna. A partir daí, compusemos os demais pacotes de samples discutidos na seção \ref{sec:trad}

Nós queríamos assegurar a acessibilidade da audiência e garantir uma flexibilidade na montagem da performance, garantindo seu funcionamento mesmo em locais sem disponibilidade de acesso à Internet, o que era o caso do Festival Bigorna. Para isso, utilizamos um roteador Wifi para criar uma rede local aberta onde os usuários podiam ter acesso ao sistema em um servidor local (esta prática é frequentemente utilizada como estratégia para permitir a participação da audiência em sistemas de música móvel\cite{Lambert:2016}). Uma questão em performances participativas via celulares é que muitas vezes os usuários podem se distrair durante a se usar internet em dispositivos celulares para performances participativas e que usuários frequentemente podem se distrair por informações vindas de outros aplicativos como de trocas de mensagens ou redes sociais\cite{wu2017open}, e isto também é evitado pela rede local, que isola os usuários da internet, deixando os mais focados na performance e na experiência resultante. Durante as performances, os participantes acessaram o sistema através de um endereço IP que direcionava para uma máquina local com o servidor.

Nossa intenção era de que os participantes pudessem interagir com pouca ou nenhuma explicação anterior. No começo das era passada somente instruções de qual endereço acessar, onde havia a informação ``envie sua mensagem''. O endereço IP e o nome da rede eram informado em uma projeção na tela (na maioria das performances) para auxiliar os participantes a entrar mesmo depois do início da mesma. Um site com uma versão online é mantido no ar desde o lançamento do projeto e pode ser acessado no endereço: \url{banda.codigo.xyz}




\subsubsection{Tradução Inter Semiótica}
\label{sec:trad}

O uso do teclado do computador como \emph{input} para produzir sons em instrumentos musicais digitais pode ser relacionado muitas vezes com o teclado de piano. A maioria dos softwares do tipo DAW, como o ``Logic Audio" ou o ``Pro Tools" propõe uma interface de ``digitação musical" onde o teclado pode ser usado como um controlador MIDI precário (no sentido de que não possui sensibilidade à pressão) que relaciona teclas a notas musicais determinadas. Neste paradigma, a associação entre letras e sons é geralmente arbitrariamente determinada pela posição das teclas do teclado em comparação com a posição das teclas do piano, sem nenhuma relação semântica entre o som gerado e as próprias letras. Neste projeto, queríamos manter uma relação semântica entre as letras utilizadas nas mensagens e o resultado sonoro gerado. Para isso, buscamos algumas formas de tradução inter-semiótica entre os caracteres e os sons, que fosse além dessa analogia com as teclas de piano. A tradução semiótica é um procedimento utilizado para traduzir diferentes códigos, como aponta Plaza: 

\begin{citacao}
Na tradução intersemiótica como transcriação de formas o que se visa é penetrar pelas entranhas dos diferentes signos, buscando iluminar suas relações estruturais, pois são essas relações que mais interessam quando se trata de focalizar os procedimentos que regem a tradução. Traduzir criativamente é, sobretudo, inteligir estruturas que visam à transformação de formas. \cite[71]{JulioPlaza1969}
\end{citacao}

Aplicar um processo de tradução inter-semiótica de texto para som se relaciona com a idéia de áudio semântico \cite{Kostek:2010}, mas não no sentido tradicional, já que o objetivo aqui não foi o de dar sentido semântico aos sons, mas sim o de explorar quais poderiam ser os significados das letras quando essas fossem transformadas em sons. A abordagem foi de produzir os samples de acordo com o conceito de James Tenney \cite{Tenney1988} de Clang, ou ``gestalt aural", que é similar ao conceito de objeto celular de Pierre Schaeffer, que são objetos sonoros breves ou redundantes \cite{Chion1983}. A ideia foi pensar em átomos que pudessem ser recombinados livremente na medida em que as mensagens fossem escritas. Este procedimento aumenta as possibilidades paratáticas do sistema ao permitir re-combinações para além das possibilitadas pela linguagem escrita.


No sistema fonético, Schaeffer \cite{Schaeffer2007} aponta que as vogais em geral servem como sons de suporte e as consoantes como articulação. Segui esse princípio guia para o mapeamento das letras em sons, buscando cobrir uma grande variação de ``clangs", para deixar espaço para o acaso. Para a primeira versão do projeto, foram feitos 4 grupos de samples com sonoridades diferentes que podem ser mudados em tempo real por quem souber os comandos especiais: 

\begin{description}
 \item[Galáxias] A primeira estratégia foi de construir um alfabeto sonoro, usando processo de analogia com os sons do alfabeto fonético. Apesar de existirem muitos sistemas de síntese de discurso, a tentativa foi de isolar os sons, atomizando o discurso e consequentemente, o transformando em música através do encadeamento sucessivo de sons em uma estrutura rítmica. 
Em uma homenagem à poesia concreta, como definida por Augusto de Campos: ``tensão de palaras-coisas no espaço-tempo'' \cite[45]{campos_teoria_2014}, começamos a recortar os sons desse alfabeto a partir das leituras de Haroldo de Campos do Poema Galáxias \cite{Campos2004}, um poema épico longo que se situa entre a prosa e a poesia, com várias páginas de texto sem nenhuma divisão de parágrafos ou marcas de pontuação. Sua leitura calma e grave foi percebida como uma fonte rica para um conjunto consistente de sons, e dessas gravações conseguimos extrair um conjunto que cobriu todas as letras em caixa baixa. Para as letras maiúsculas, procurei buscar sons mais poderosos, parte dos quais retirei de uma demonstração de técnicas vocais estendidas gravada por Stênio Biazon e outros que gravei especialmente para isso usando minha própria voz.  

Para os caracteres, a relação direta entre os sons e os samples não é tão clara, uma vez que não são representações fonéticas, então optei por procurar outras relações possíveis bem como outras fontes de sons. Em alguns casos seguimos uma lógica de relação mais simbólica, como de caixa registradora para o signo \$ e em outros, associações mais icônicas, utilizando uma técnica de síntese subtrativa por corte visual de espectro. Este é o conjunto padrão de samples, e é usado como introdução para tornar as pessoas familiares com o sistema de associação. Exemplos deste pacote de sons podem ser ouvidos em: \url{http://spectro.codigo.xyz/spectrogramplayer/}

\begin{figure}[htb]
    \caption{\label{samplesgalaxias}Espectrogramas do Conjunto de Samples Galáxias}
    \begin{center}
        \includegraphics[width=0.7\linewidth]{pictures/cap3/bandagalaxias.jpg}
    \end{center}
    \legend{Fonte: screenshot da autora}
\end{figure}


\item[Percussão e Acordes] Para o segundo conjunto de samples, procurei lidar com materiais mais tradicionais do repertório da música popular eletrônica, como samples de percussão e acordes de sintetizadores extraídos de bancos de samples. Para manter uma estrutura lógica coerente na relação entre sons e letras, sons percussivos foram empregados no papel de articulação (nas consoantes) e acordes de sons contínuos como suporte (nas vogais). Nesta experiência, usamos uma progressão de Dó maior para a composição dos acordes das vogais, com samples gerados por síntese aditiva, e para as letras maiúsculas utilizamos um timbre com mais harmônicos do que os das letras em caixa baixa. \footnote{Exemplos deste pacote podem ser ouvidos em: \url{http://spectro.codigo.xyz/spectrogramplayer/perc.html} }
A associação entre as consoantes e os sons percussivos foi feita através de uma escuta reduzida de uma coleção grande de samples, buscando identificar sons que lembrassem características fonéticas das letras originais, como um bumbo para a letra ``B'' e pratos para ``S''. Mais uma vez aqui, fizemos essa associação mais direta entre sons e fonemas nos caracteres do alfabeto, enquanto para os caracteres especiais procuramos outras relações, como por exemplo entre sua forma visual e o desenho espectral dos sons. Alguns caracteres especiais foram extraídos da peça de Velimir Khleibnikov ``Radio of the Future''\footnote{Disponível em: \url{http://www.ubu.com/sound/russian_avant.html}}.

\begin{figure}[htb]
    \caption{\label{samplespercussao}Espectrogramas do Conjunto de Samples Percussão e Acordes}
    \begin{center}
        \includegraphics[width=0.7\linewidth]{pictures/cap3/bandapercussao.jpg}
    \end{center}
    \legend{Fonte: screenshot da autora}
\end{figure}

\item[Pacote Colaborativo]O terceiro conjunto de samples foi montado a partir da colaboração de alunos da turma de Produção Musical da Universidade Anhembi Morumbi, sob orientação do professor Viktor Kisil, que solicitou da turma samples para a composição deste conjunto de sons. Este pacote contém samples de fontes sonoras muito variadas, de concretos a sintetizados. A associação entre letras e sons não foi feita procurando analogias como nos outros conjuntos, e sendo assim, ele provém sons mais excêntricos. \footnote{Exemplos podem ser ouvidos em: \url{http://spectro.codigo.xyz/spectrogramplayer/fx.html}} 

\begin{figure}[htb]
    \caption{\label{samplescolab}Espectrogramas do Conjunto de Samples Colaborativo}
    \begin{center}
        \includegraphics[width=0.7\linewidth]{pictures/cap3/bandaabertacolab.jpg}
    \end{center}
    \legend{Fonte: screenshot da autora}
\end{figure}

\item[Orquestra Errante] O quarto conjunto é formado por samples recortados de gravações de sessões de improvisação musical da Orquestra Errante, conduzida pelo professor Rogério Costa. Eles foram produzidos em sua maioria por instrumentos acústicos, como saxofone, flauta, piano, trombone, contrabaixo, percussão e voz. As gravações base que utilizei foram feitas durante ensaios da orquestra no estúdio do CMU, muitas vezes utilizando técnicas estendidas. Nós obtivemos um arquivo com várias gravações passadas desses ensaios, e empregamos técnicas de escuta reduzida e análise espectrográfica pra identificar ``clangs" que pudessem ser isolados para funcionar como átomos. Aqui novamente aplicamos uma analogia entre os fonemas e os sons sampleados e as funções de articulação para as consoantes e de suporte para as vogais. Para os numerais, usamos analogia entre batimentos e a quantidade representada. Exemplos deste conjunto podem ser ouvidos em: \url{http://spectro.codigo.xyz/spectrogramplayer/orquestra.html}
\end{description}

\begin{figure}[htb]
    \caption{\label{samplesorquestra}Espectrogramas do Conjunto de Samples Orquestra Errante}
    \begin{center}
        \includegraphics[width=0.7\linewidth]{pictures/cap3/bandaorquestra.jpg}
    \end{center}
    \legend{Fonte: screenshot da autora}
\end{figure}

%It is important to emphasize that the browser used by the participants can make a real difference on the performance itself. Most current browsers do enable web audio synthesis, however they are incompatible with many parameters. That said, the participants are recommended to use Google Chrome and take advantage of its full compatibility with current web audio standards.
\subsection{Segunda Versão: Web Audio Type -- Tipografia sonora}
Em uma segunda versão do projeto \cite{Stolfi2017w}, onde nosso objetivo era testar uma versão totalmente baseada em síntese sonora no browser, procuramos explorar uma abordagem diferente para o processo de tradução intersemiótica. Nesta segunda versão, desenvolvemos um sistema de analogia direta entre as formas das letras e o perfil espectral dos sons gerados, criando um ``alfabeto sonoro'' que ao mesmo tempo em que soava, desenhava a forma do texto no espectro. Para tanto, fizemos uma associação entre osciladores -- que desenhavam as linhas horizontais e as inclinadas -- e um sintetizador de rúido baseado em FFT que foi desenvolvido especialmente para o projeto -- que construía os blocos verticais das letras desse alfabeto sonoro.

Durante o desenvolvimento desta nova versão, nós optamos por avaliar a possibilidade de integração com algumas soluções pré programadas para síntese sonora via web, como os \emph{frameworks} Gibber \cite{Roberts2012gibberlivecoding}, Waax~\cite{Choi2013waax}, e meSpeak.js~\footnote{meSpeak.js website: \url{http://www.masswerk.at/mespeak/}}. Utilizar soluções prontas para síntese sonora poderia faciliar o uso e compatibilidade da tecnologia, mas ao mesmo tempo também criaram certas barreiras e imporiam algumas limitações que não são oferecidas pelo uso de web Audio ``puro''. Além disso, alguns desses frameworks ofereciam muito mais recursos do que o necessário pelo projeto, que poderia dificultar o processo de desenvolvimento. Nós tentamos usar o framework meSpeak como uma ferramenta simples para conversão de texto em discurso, mas o resultado sonoro final não foi musicalmente satisfatório, e ele não permitia que se tocassem mensagens umas em cima das outras. No final, decidimos desenvolver uma solução toda em Web Audio, para a programação do processamento de áudio.



%During the development of this new version we opted to evaluate some ready-made solutions for web audio synthesis like Gibber~\cite{Roberts2012gibberlivecoding}, Waax~\cite{Choi2013waax}, and meSpeak.js~\footnote{meSpeak.js website: \url{http://www.masswerk.at/mespeak/}}.
%These solutions are able to facilitate the use of web audio technology and other browser facilities, but also create barriers and sometimes limitations that weren't imposed by pure web audio. Furthermore, sometimes frameworks could offer more than what was required by the application, which can lead to more confusion during development. We tried to use, meSpeak.js as a simple and interesting text-to-speech library but the resulting sound was not musically satisfactory, and we couldn't make messages play one on top of each other, due to limitations of the framework, so we choose to focus on pure web audio for sound programming.

Após as primeiras experiências com síntese de voz, decidimos por uma outra abordagem estética, baseadas em conceitos de design tipográfico \cite{ruder_typography:_2009} para propor uma ``tipografia sonora'', e também para experimentar com novas formas de fazer música, evitando acordes e escalas tradicionais da música ocidental. Essa idéia guiou toda a proposta de desenvolvimeto dessa nova versão e causou um grande impacto no resultado obtido.

Essa idéia de ``tipografia sonora'' já havia sido explorada em um trabalho anterior que eu desenvolvi em 2014, chamado ``Utopia'', onde trabalhei a com síntese subtrativa a partir de uma base ruidosa, que era a gravação de uma serra de fita em atividade, para desenhar a palavra UTOPIA no espectro sonoro. Nesse caso, bem como na versão em Web Audio do Banda Aberta, nós também partimos de uma idéia de tradução intersemiótica, assim como na outra versão, mas aqui, procuramos uma relação de isomorfia entre o desenho das letras e som, dispensando a relação fonética. Para tanto, projetamos ``letras'' formadas por blocos de sons pré-determinados, através de síntese aditiva e síntese de ruído, para sonificar o desenho das letras do alfabeto.



%We also decided to follow some aesthetic concepts based on type design~\cite{ruder_typography:_2009} to propose ``an audio typograph", and also to experiment with new kind of music, avoiding traditional chords and scales. These concepts surrounded the whole development of this current version of Open Band and had a great impact in the new result obtained.

%In this particular version, we are also using a concept of audio typography as base to this inter-semiotic translation, to decode letters into sounds, building spectral designed ``letters" made of predetermined sound blocks. Trough this path, we are abstracting the aural aspects of the typed characters and grabbing only the shape of the letters as information for the sound synthesis.

%The effect of typography on written texts acted as a motivation in the present artifact to experiment with ways in which timbre could affect the sound produced by musicians. We use additive and noise synthesis to sonify ``drawings" of the shapes of the letters.


\begin{figure}[!ht]
    \centering
        \includegraphics[width=1\linewidth]{pictures/metamagical}
        \vspace{-10pt}
    \caption{One of the modular alphabets proposed by Douglas Hoefstader in the book Metamagical Themas, page 90}
    \label{fig:metamagical}
\end{figure} 

Um dos pontos de partida para essa idéia foi o trabalho de Donald Knuth, um cientista da computação que trabalhou com questões de tipografia, e desenvolveu o conceito da ``meta-fonte'', um tipo que não era desenhado estaticamente, mas cujo desenho poderia variar de acordo com parâmetros tipográficos\cite{knuth-meta-font_1982}. A partir do design dessa ``meta-fonte'', muitas famílias tipográficas diferentes poderiam ser geradas, simplesmente pela mudança de parâmetros como: altura, largura, espessura, linha de base, altura de x etc. Como aponta Douglas Hesfstader:

\begin{citacao}
Knuth's purpose is not to give the ultimate parametrization of the letters of the alphabet (indeed, I suspect that he would be the first to laugh at the very notion), but to allow a user to make ``knobbed letters" -- we could call them letter schemas. This means that you can choose for yourself what the variable aspects of a letter are, and then, with Metafont's aid, you can easily construct knobs that allow those aspects to vary. 
(...)
Knuth's purpose is not to give the ultimate parametrization of the letters of the alphabet (indeed, I suspect that he would be the first to laugh at the very notion), but to allow a user to make ``knobbed letters" -- we could call them letter schemas. This means that you can choose for yourself what the variable aspects of a letter are, and then, with Metafont's aid, you can easily construct knobs that allow those aspects to vary~\cite{Metamagical1986}. 
\end{citacao}

%Donald Knuth, a computer scientist and artist who worked on typesetting issues, developed the concept of a ``meta-font", a typeface that was not statically drawn, but that could be changed trough typographical parameters\cite{knuth-meta-font_1982}. Following the rules of meta-fonts, many different typefaces could be generated simply by varying the type parameters. As says Douglas Hoefstader in ~\cite{Metamagical1986}, \textit{``Knuth's purpose is not to give the ultimate parametrization of the letters of the alphabet (indeed, I suspect that he would be the first to laugh at the very notion), but to allow a user to make ``knobbed letters" -- we could call them letter schemas. This means that you can choose for yourself what the variable aspects of a letter are, and then, with Metafont's aid, you can easily construct knobs that allow those aspects to vary."}.





\begin{figure}[!ht]
   \centering
       \includegraphics[width=1\linewidth]{pictures/audiotype_sketch}
   \caption{Sketch for the modular font to be base for audio synthesis.}
    \label{fig:sketch}
 \end{figure}


%\begin{figure}[!hb]
%   \centering
%       \includegraphics[width=0.45\textwidth]{pictures/audiotype_v1_2_2}
%       \vspace{-10pt}
%   \caption{Project for audio typography, made of blocks of noise, sine waves and glissandi. The red blocks represent noise and the black lines solenoids and glissandi.}
%   \vspace{-10pt}
 %   \label{fig:project}
%\end{figure}

Nós usamos alguns desses parâmetros tipográficos definidos por Knuth para estabelecer frequências geradoras para os sons puros: as frequências mais baixas corresponderiam às linhas dos descendentes, a linha de base para a segunda, a altura de x para a terceira e a altura da caixa para a quarta frequência, em ordem ascendente. Os parâmetros horizontais, por sua vez, foram convertidos em medidas temporais, para determinar a duração dos eventos sonoros e dos intervalos entre as letras.

%We used the basic parameters defined by Knuth~\cite{Metamagical1986} such as x-height, baseline, height of ascendants and descendants, as measures to establish frequencies for tone sounds (one low, one at the baseline, one at the x-height an one at the uppercase line of each letter). The horizontal parameters of the type are translated into temporal measures, to determine duration of the sound events and intervals between the letters.   

A meta-font proposta por Knuth é uma fonte complexa com cerca de 36 parâmetros tipográficos variáveis. Para a nossa ``tipografia sonora'', no entanto, propus uma analogia mais simples, semelhante às propostas por Douglas Hoefstader no seu livro ``Metamagical Themas''\cite{Metamagical1986}(ver Figura \ref{fig:metamagical}), baseada em uma estrutura de grelha. Na nossa proposta, as letras seriam desenhadas a partir de poucos elementos básicos, como no rascunho apresentado na figura \ref{fig:sketch}. As linhas das letras foram mapeadas em dois tipos de funções: síntese de noise (noise())baseada em FFT para os blocos verticais, e senóides (sine()) para as linhas horizontais e diagonais (que soam como glissandos), como pode ser visto nas Figuras \ref{fig:noise} e \ref{fig:sine}. 


%A meta-font is a complex font with several typographic parameters. In this work, we are using a simpler analogy by proposing a modular type, similar to the ones Douglas Hoefstader~\cite{Metamagical1986} proposes in his book ``Metamagical Themas", based on simple grid structures (see Figure \ref{fig:metamagical}). We propose a model that can work only with a few building blocks (see Figure in which they are sketched). The line of the letters was mapped into two basic types of functions: noise synthesis for the verticals and solenoids for the horizontal lines and glissandi, as defined by the composer project in Figure x.


A função ``sine()'' tem um \emph{buffer} de oscilador e podem gerar frequências estáticas ou em rampas lineares, deste modo, geram linhas horizontais ou diagonais no espectro. As funções ``noise()'' geram blocos de ruído que vão de faixas de frequência determinadas. Como processo de programação, definimos 20 funções específicas para cada módulo das letras e cada letra é então formada pelos seus módulos correspondentes. 





%Sine function has one oscillator buffer that can have static frequencies or linearly ramping frequencies. This creates diagonal and horizontal lines in the spectrum. Then, frequencies are defined as the bottom and top limit, some steps are defined in the middle. The length of spectral letters are as well defined. By the end, letters are translated as a group of functions, and the result is played one after another. We built 20 predetermined functions to correspond at each ``type block", and each letter plays as many functions as are determined by the typeface project. Thus, we created a system of mapping letters to the functions.

Cada nó de saída de áudio passa também por um envelope ADSR \cite{Lee2016} para gerar uma forma mais natural de ataque e release, e também passa por um nó de ganho que pode ser controlado durante a performance. A figura \ref{fig:spectro}, mostra um espectrograma gerado pelo sistema com os resultados obtidos.

%Every output node goes through an ADSR class \cite{Lee2016} that creates a more natural attack/release, and also goes through a gain node, that can be changed during a performance. On Figure \ref{fig:spectro} is shown a spectrogram generated by the application with the results obtained.

\begin{figure}[!ht]
    \centering
        \includegraphics[width=1\textwidth]{pictures/audiotype_v1_noise}
        \vspace{-10pt}
    \caption{Functions for playing vertical blocks with noise synthesis}
    \vspace{10pt}
    \label{fig:noise}
        \centering
        \includegraphics[width=1\textwidth]{pictures/audiotype_v1_sine}
    \caption{Functions for playing horizontal and diagonal lines with oscillators}
    \vspace{-10pt}
    \label{fig:sine}
\vspace{10pt}
\end{figure}

\begin{figure}[!ht]
    \centering
        \includegraphics[width=1\textwidth]{pictures/spectrogram}
        \vspace{-10pt}
    \caption{Spectrograms generated by the application}
    \vspace{-10pt}
    \label{fig:spectro}
\end{figure}

%As every letter from every message is translated into a group of playing Web Audio nodes, the number of oscillators that plays simultaneously can grow really quickly depending on the number of participants sending messages. As this application was intended to play in many different mobile devices, this led to the necessity of limiting the creation of nodes, as this could take more processing than some devices could take, and in some cases could make the device stop playing, which is undesirable behavior for such application.






\subsection{Apresentações públicas}
Nós apresentamos o Banda aberta em diferentes ocasiões, para diferentes perfis de público. Dependendo do perfil dos participantes, a conversa foi para direções diferentes. Em uma primeira fase, apresentamos o projeto rodando em um servidor local, que era acessado através do endereço IP, para reduzir a latência e manter o público reunido em uma mesma rede. Posteriormente, passamos a usar a versão online do sistema em outras performances, para facilitar o processo de montagem. Até o momento, realizamos as performance seguintes:

\begin{description}

\item[Ensaio no Nusom] 
A primeira apresentação informal do projeto aconteceu na reunião do NuSom do dia 9 de maio de 2016. O público presente era de pesquisadores do grupo, e cerca de 9 pessoas participaram da experiência. Notamos uma diferença considerável de tocabilidade entre quem estava com laptop em relação a quem estava com celulares. O site também não funcionava em todos celulares, especialmente modelos de iphone mais antigos. \footnote{Registro da apresentação em \url{https://www.youtube.com/watch?v=Utc_4mT5b8s}}

\item[Festival Bigorna]
No dia 26 de junho de 2016, estreamos publicamente o projeto no Festival Bigorna, que aconteceu na Praça José Molina, próxima à Avenida Paulista em São Paulo. O Estúdio Fita Crepe, um pequeno espaço dedicado à produção e difusão de música experimental organizou o festival que ocupava essa praça considerada subutilizada, pela sua localização\footnote{A programação do festival está disponível em: \url{http://www.festivalbigorna.com/2016/}}.

\begin{figure}[!ht]
    \centering
        \includegraphics[width=1\textwidth]{pictures/bigorna}
        \vspace{-10pt}
    \caption{people interacting within the piece}
    \label{fig:performer}
\end{figure}

O público presente no momento era de cerca de 80 pessoas, mais do que o roteador podia comportar (30 pessoas ao mesmo tempo), o que deixou várias pessoas de fora da interação. A audiência incluía membros do NuSom, músicos da cena experimental e o público geral do festival, mas grande parte era formada pelos estudantes de produção musical que colaboraram com o terceiro conjunto de samples. Jovens, ficaram bastante eufóricos com a possibilidade do anonimato e praticaram um certo nível de \emph{bulling} entre eles. 

\item[Rádio grave] 30 - 6 -2016
Fomos fazer uma performance também na Rádio Grave, uma rádio web que foi montada no Gfau para ajudar na mobilização dos  estudantes durante a greve do ano passado. Os locutores simulavam que rádio era uma nave espacial, e a ``aterrissagem'' do Banda Aberta por lá rendeu um episódio que ficou conhecido como ``reset do universo'', pela surpresa gerada pelos sons fora do comum. Uma gravação do áudio da performance pode ser ouvido no seguinte link: https://soundcloud.com/asss/banda-aberta-na-radiograve-reset-do-universo.


\item[¿Musica? 12]
No festival, que faz parte de uma série organizada pelo NuSom, apresentamos pela primeira vez a segunda versão do projeto, com síntese sonora em Web Audio, para um público formado principalmente por alunos e professores da Música. 
\url{https://www.youtube.com/watch?v=hRELFhQm6M0&t=639s}


\item[Web Audio Conference] 


\item[Female Laptop Orchestra]
Na performance ``Trasnmusiking'', realizada pelo grupo Female Laptop Orchestra, utilizamos o projeto Banda Aberta em uma apresentação realizada em Londres durante a conferência ``Audio Mostly'' de 2017. A peça contou com a participação remota de 12 pessas, que mandavam streams de áudio de lugares diferentes do globo, esses streams eram mixados em tempo real por Anna Xambó. Além disso, utilizamos o Banda aberta em uma versão com síntese por web Audio, mas somente de ruído, para receber feedback em tempo real da audiência, e também performei com técnicas vocais estendidas e percussão. A audiência era formada principalmente por participantes da conferência, em sua maioria acadêmicos da área de tecnologia musical. A audiência utilizou o chat para dar opiniões sobre a performance e também para piadas com membros da equipe, principalmente Alo, que era um dos organizadores da parte musical.


\item[Audio Mostly Workshop]
Durante o workshop Sounds in the Cloud, organizado em um trabalho conjunto pela equipe do projeto Audio Commons da Queen Mary University of London (QMUL) e pela equipe de pesquisadores do sistema de \emph{Machine Learning} RapidMix da Universidade Golsdmith, os participantes foram convidados a explorar ferramentas desenvolvidas pelos pesquisadores em grupos. Nossa equipe desenvolveu uma versão do projeto Banda Aberta que utilizava a ferramenta RapidMix para mudar parâmetros sonoros através da captura de movimentos pela câmera. Com isso, era possível treinar o sistema para reconhecer certos movimentos em tempo real. Com a experiência, foi possível verificar tanto a facilidade de adaptação do nosso sistema para incorporar tecnologias ded terceiros, quanto a facilidade de utilização do sistema RapidMix para ser incorporado em demais projetos de interação através de sua API em JavaScript.

\item[Encontro da rede de Tecnoxamanismo] 12-8-2016
O encontro da Rede de Tecnoxamanismo na Dinamarca aconteceu na cidade de Aarhus, no ``Dome of visions'', um domo geodésico na região portuária da cidade, onde acontecem uma série de eventos ligados à sustentabilidade, agroecologia, ecologia e temas afins. Além de colaborar na produção do Evento, apresentei a performance para um público que incuía músicos, produtores, performers e interessados de uma maneira geral. O público foi muito comportado no sentido de não escrever muitas bobagens no chat. Nesta performance, utilizamos a versão online da ferramenta, sem o servidor local.


\item[SHA Festival] 7-8-2017
O Festival ``Still Hacking Anyway'' é um encontro de hacker, programadores, pensadores e ativistas que acontece de quatro em quatro anos na Europa. Na edição de 2017 do evento, apresentamos o projeto em uma performance de 45 minutos, onde participaram interessados de diversas partes do mundo. Por ser um festival hacker, a maior parte do público portava laptops ao invés de celulares, o que permitiu uma maior agilidade na digitação das mensagens. Alguns dos presentes passaram a performance tentando hackear com o intuito de derrubar o sistema, o que ocorreu a cerca de 40 minutos depois do início, através de um script que sobrecarregou o sistema e causou uma pane no audio buffer. Pelas características do evento, que era um festival hacker,  já esperávamos um comportamento deste tipo por parte da audiência, que ocorreu até mais devagar do que o esperado. O ataque, no entanto, não causou nenhum tipo de dano ao servidor, que voltou ao normal logo que a performance se encerrou.

\item[aMostra Sonora] 2-7-2016
Na apresentação que fiz solo no festival aMostra Sonora em 2016, utilizei um patch de Pure Data que venho desenvolvendo desde 2006, chamado Modulari, que se baseia em samplers, sintetizadores por \emph{waveshaping}, efeitos para o microfone e osciladores, em conjunto com o Banda aberta. Durante a performance, que durou 40 minutos, utilizei os dois sistemas e técnicas vocais expandidas em conjunto. Apesar de ter corrido normalmente, considerei que os resultados sonoros do conjunto ainda eram limitados, na possibilidade de variação sonora, e passei a planejar um sistema alternativo que fosse mais rico também para performances solo, que descreverei na próxima seção desta tese.




\item[Congresso da Abrapem]

No congresso da Associação Brasileira de Performance musical (30 de junho de 2016), o público de cerca de 23 pessoas, era majoritariamente formado por músicos tradicionais, que ficaram perguntando sobre as referências musicais, de onde vieram os samples, e experimentaram bastante com possibilidades rítmicas. Um registro da performance pode ser visto em:  \url{https://www.youtube.com/watch?v=NOWapLq6eiU}.



\item[Concerto de Computação Musical no IME] (22-9-2016)
No concerto de computação musical no IME aconteceu uma coisa inusitada, o público era formado majoritariamente por programadores e um dos participantes percebeu que era possível inserir comandos em HTML e CSS através do chat. Logo, os participantes começaram a encher a tela com formas geométricas e letras em movimento, e ficaram eufóricos com essa possibilidades de hackear o chat. A gravação da tela da apresentação pode ser vista em: https://www.youtube.com/watch?v=xs23z1IfPfY. A facilidade de uso levou a um grande engajamento do público durante as performances, e traz também um grande nível de divertimento, aproximando assim, como desejávamos, atividade musical de um contexto mais lúdico como o de jogo.

\item[Áudio Insurgência] (7-10-2016)

No festival Áudio insurgência (8 de outubro de 2016), o público era formado principalmente por membros e entusiastas da cena de música experimental e noise de São Paulo. A mesa de som da casa estava com defeito, então ligamos ligamos o celular diretamente nas caixas de som. Também não havia projeção, então não se estabeleceu um ponto focal, e as pessoas ocuparam todos os espaços da casa com seus dispositivos. A gravação do chat pode ser vista em: https://www.youtube.com/watch?v=DpCuU41tWM8 

\end{description}







\section{Análise preliminar}

%comment: In section 4, the authors allude to issues of cross-browser compatibility, but in 3.3.8 they suggest it "is working correctly" on all major browsers. This seeming contradiction needs to be resolved, and hopefully in a way that explains in more detail the issues encountered, the solutions, and any broader take-aways in terms of the future of web audio API.

Em uma análise inicial, foi perceptível que em contextos onde a audiência tinha mais relação com o meio musical, havia uma tendência maior dos participantes passarem mais tempo experimentando com os sons e com o ritmo, digitando mensagens sem significado discursivo. Quando o público era mais jovem, os participantes tendiam a jogar mais com a possibilidade de escrever anonimamente. Quando as conversações começavam a esquentas, camadas de sons eram sobrepostas, tornando o ritmo mais frenético.

Como em uma conversa real, se as pessoas não param para ouvir uns aos outros, a comunicação se torna mais confusa. Nós recebemos através do chat, durante as performances, uma boa quantidade de \emph{feedback} dos usuários, com pessoas perguntando tanto sobre o sistema quanto sobre os sons, e elogiando a performance.

%We observed that in musical contexts, the audience keeps more time experiencing with the samples and rhythm, typing meaningless phrases, and when the public is younger, the participants tend to play with the ability to speak anonymously. 
%As the conversation ``warms" up, layers of sounds are overlapped, turning the rhythm into something more frenetic. 

%Like in a real conversation, if people don't take time to listen to the others, the sound becomes more difficult to distinguish. We received a lot of feedback from the public on the chat, with people asking things about the project and supporting it.

Um problema da primeira versão do projeto era de que ela era baseada em um volume grande de dados dos samples (cerca de 60 mB), então era necessário criar uma rede local, pelo menos no Brasil, onde a qualidade das redes de internet é menor, e mesmo assim o sistema ainda demorava significativamente para carregar. Na segunda versão do projeto, que era baseada em síntese, o volume de dados foi reduzido drasticamente, para apenas alguns kilobytes, o que reduziu também o tempo de carregamento do sistema. O uso de dados durante a performance, no entanto é baixo em ambas versões, uma vez que não há transmissão de áudio entre os usuários, somente de mensagens de texto, que em geral não são muito pesadas. 

%One problem of the previous version was that been based on heavy volume of data from samples, we had always to create a local network and even with that we had a long time of download before playing. With the present version relying only on web audio synthesis, the size of the project reduced dramatically --- from 60 megabytes to kilobytes --- and also the downloading time when compared with the previous version.
%This new aspect of the Open Band facilitates users to participate using 4G data plan as the data consumption is now comprised of text messages only as no samples files are downloaded. 

É importante enfatizar que o navegador utilizado pelos participantes pode causar uma diferença real durante as performances. A maioria dos navegadores atuais já suporta a Web Audio API, mas alguns ainda são incompatíveis com certos parâmetros. Por este motivo, recomendamos a utilização do navegador Google Chrome, que é atualmente o mais compatível com os padrões definidos pelo W3C \footnote{W3C é a organização que define os padrões para as linguagens base da web, como HTML, CSS, JavaScript etc.}


%


\subsection{Análise da interação com a audiência}
%As described before, one of the design aims of the project is to facilitate participatory musical experiences for audiences who don't necessarily have a musical background. We aim to engage participants in situated playful interactions. In this Section we first describe the Open Band performances and then present our evaluation methodology.

Seguida a uma análise preliminar, procuramos fazer uma análise mais sistemática de algumas das performances apresentadas. Se olharmos para o projeto da perspectiva de um trabalho artístico, nós procuramos analisá-lo como ``uma prática participativa, culturalmente posicionada em sem regras e gradações explícitas''\cite{McCullough1998}, ao invés de procurar analisar o projeto em termos de usabilidade, que seria um critério mais adequado a produtos de design. No entanto, nós procuramos buscar evidências de apreciação e engajamento. Uma das formas possíveis de se analisar segundo esses critérios foi analisando os registros de dados das mensagens enviadas durante as performances apresentadas ao público. Pensando no projeto como um aparato de comunicação, nós estávamos interessados em investigar a natureza dessas interações multi-usuários que aconteceram através do sistema de chat em alguns dos contextos dados. Para isso, procuramos investigar padrões linguísticos, para verificar como e se eles se relacionariam também com as interações sonoras.


%When looking at the project from the perspective of an artwork, we are interested in analysing it as ``a participatory practice, culturally positioned, and without explicit rules or grading" \cite{McCullough1998}, rather than in terms of usability, a dimension more adapted to product design. We however also aspire to search for evidence of appreciation and engagement which have to do with usability, to an extent. One of the access points to study how people engaged in the Open Band performances is to analyse data logs of the messages that were sent during interactions. As a communication device, we are interested in the nature of the multi-user interactions occurring through the chat system in given contexts. We namely want to investigate linguistic patterns and if and how they are inter-related to specific audio interactions.

Para essa análise mais detalhada, contamos com o auxílio de um pós graduando em Ciência da Computação, Janis Sokolovskis, além do supervisor do meu estágio na QMUL, Mathieu Barthet. Nó utilizamos um método misto que combinou análises qualitativas e quantitativas dos padrões de interação durante as performances. Analisando a frequência e o conteúdo das mensagens, procuramos descobrir dados a respeito do engajamento da audiência e de como os participantes utilizaram sua liberdade de expressão em um contexto de interação sonora. 

%We use a mixed methods approach by combining qualitative analyses of semantic content, and quantitative analyses of interaction patterns. By analyzing the frequency and the content of the messages sent by participants, we hope to infer knowledge about their degree of implication in the performance and how they articulate free expression in the context of sonic interactions. 

Para essa avaliação, avaliamos quatro das primeiras performances públicas do projeto, envolvendo cerca de 100 participantes de diferentes idades e perfis Elas ocorreram no Brasil em 2016 como parte de festivais e conferências: a performance durante o Festival Bigorna na Praça (p1), no Congresso da ABRAPEM (p2), no Concerto de Computação Musical no IME (p3) e no festival Áudio Insurgência (p4) e. A mesma versão do software foi utilizada em todas elas com alguns pequenos ajuste para correção de pequenos \emph{bugs}.







\begin{table*}[ht!]
%\tabcolsep8.1pt
\ABNTEXfontereduzida
\setlength\extrarowheight{-2pt}
\caption{Participação no projeto Banda Aberta, inspirada pelas dimensões de participação propostas por Wu \cite{wu2017open}}{%
\begin{tabular}{p{3cm}p{3cm}p{4cm}}
\hline
\textbf{Dimensão } & \textbf{Banda Aberta} & \textbf{Descrição} \\
Nível de agência & Média & Participantes têm os mesmos controles, mas os condutores têm controles específicos.\\
Interação Social & Ação Conjunta & Mensagens de texto podem ser mandadas simultaneamente ou não.\\
Agência social  & Nível individual & A performance é resultante a contribuição dos participantes individualmente.\\
Mediação da agência & Direta & Os inputs da audiência afetam diretamente os sons tocados. \\
Narrativa & Centrada na audiência & Toda performance é resultado das interações da audiência.\\
Constrições & Escolha limitada de sons & Os pacotes de samples são pré-compostos e escolhidos pelo condutor.\\
Mídia & Audio e Visual (chat) & \\
Oportunidade criativa & Expressão linguística & O controle do fluxo sonoro é dado por mensagens instantâneas através do chat.\\
Interface & Web, tela, linha de comando & \\
Situação & Co-locada & \\
\hline
%\\\botrule
\end{tabular}}
\label{tab:participation}
\end{table*}

\newpage

\subsubsection{Resultados}

O software não registrava o endereço IP dos usuários, mas funcionava atrás de mensagens que eram enviadas ao servidor, que ficam registradas em um arquivo na raiz do servidor. No código abaixo está um pequeno extrato das mensagens registradas no servidor. 


\todo[inline]{trecho de mensagens}


Para o tratamento dos dados brutos, utilizamos um script em Python para extrair o conteúdo textual das mensagens de cada performance. Nossa primeira abordagem em relação aos dados gerados pela performance foi o de gerar nuvens de tags a partir dos dados das mensagens. Em seguida, fizemos uma análise temática \cite{Braun2006} para identificar temas e tópicos recorrentes durante as performances que apresentamos na seção \ref{}. O processo de análise temática envolve a ``codificação'' do texto em temas comuns, e para isso, construímos tabelas em excel com listas de cada performance onde marcamos os códigos correspondentes a cada mensagem. Os resultados dessa análise temática será apresentado mais adiante na seção \ref{sec:thematic}. 

%We analyzed the server message logs using descriptive statistics and generated tag clouds related to each performance \footnote{Log data is formatted to send only the text and info about the type of message. A new message typed is stored as: [:message, "\{\textbackslash''text\textbackslash":\textbackslash''BASS\textbackslash",\textbackslash''touch\textbackslash":0\}'']; and a message clicked on the screen is stored as: [:message, "\{\textbackslash''text\textbackslash":\textbackslash''Som\textbackslash",\textbackslash''touch\textbackslash":1\}''].}. We then conducted a thematic analysis \cite{Braun2006} to identify recurring themes and topics during the performances.%We sorted and parsed the messages to extract semantic metadata information and analyzed the textual parts.

Como o anonimato era importante dentro da proposta do projeto, nós não guardamos dados a respeito dos usuários individuais do sistema, como endereço IP, por exemplo. Desse modo, não era possível relacionar mensagens determinadas com usuários específicos, e nem também analisar comportamentos individuais durante as performances. O que foi possível, no entanto, foi analisar a frequência e os tipos de mensagens enviadas.

%As maintaining anonymity in the message of the participants was one of the requirements in the design, we did not store any data enabling to identify users such as IP addresses. Hence we cannot relate specific messages to users. However we can analyze trends in the data through frequency and pattern analysis.

\begin{table}[ht!]
\tabcolsep8.1pt
\caption{Quantidade de mensagens, seções e número máximo de usuários simultâneos durante as performances.}{%
\begin{tabular}{@{}lc@{}}\hline
 Número de mensagens de usuários & 7322\\
 Número de sessões individuais & 865\\
 Número de usuários máximo simultâneos & 31\\
 Média de mensagens por sessão única & 8.26\\
\end{tabular}}
% \begin{tabnote}
% Quantitative data about the performance.
% \end{tabnote}
\label{tab:overallmsg}
\end{table}

A Tabela \ref{tab:overallmsg} mostra o número total de mensagens (7322) enviadas pelo chat em todas as performances. No total, houveram 865 sessões únicas que acessaram o sistema, esse número pode corresponder a mais de uma sessão por usuário. Baseado no registro dos ID das sessões, não é possível calcular o número total de usuários, mas somente o número de pessoas utilizando o sistema simultaneamente, que chegou a 31.

%Table  shows the overall number of messages (7322) sent through the chat system across all four performances. In total, there were 865 unique web sessions that accessed the Open Band application. It is not possible to infer the number of participants from the number of sessions as users may have used several sessions during a same performance with different IP addresses. Based on the number of IP addresses allocated to users by the router, the maximum number of simultaneous users in the performances reached 31.

O sistema de mensagem do projeto permite dois tipos de interação dos usuários, eles podem tanto digitar um texto e mandá-lo para o servidor, fazendo com que a mensagem chegue para os demais usuários e apareça escrito no site, ou então apenas repetir um determinado som, clicando nas mensagens já enviadas. A tabela \ref{tab:msgtype} registra o número de mensagens de cada tipo entre as performances. A duração e o número de participantes difere em cada performance, o que ajuda a explicar as diferenças na frequência de mensagens.

%The messaging process in Open Band can be twofold, either new content is written and sent to the server, or previously written messages are repeated. The latter can be done by tapping on a specific message on the mobile phone interface. Repeating a message re-triggers the sounds generated by the message. Table \ref{tab:msgtype} reports the number of messages from each type across performances. As the duration and number of participants differed in each performance, these count amongst the factors explaining the differences message frequency. 

\begin{table}[ht!]
\tabcolsep6.1pt
%\tbl{General data about the performances}{%
\caption{Número de mensagens escritas e repetidas em cada performance}{%
\begin{tabular}{@{}lccccc@{}}
\hline
Performances        & 1 & 2 & 3 & 4 & Total \\
\hline
Mensagens escritas &    1558&   849  &  594  &  723 & 3724 \\
Mensagens clicadas &    841 &   562  &  1298 &  897 & 3598 \\
Total &             2399&   1411 &  1892 & 1620 & 7322 \\
\end{tabular}}
\label{tab:msgtype}
\end{table}


Todas performances aconteceram no Brasil, portanto a maior parte das mensagens foi escrita em português. Antes de fazer a análise temática das mensagens, fizemos uma análise computacional utilizando técnicas de processamento de linguagem natural com auxílio de scripts em Python\footnote{http://www.nltk.org/}. 

A Tabela \ref{tab:words} relata o número de \emph{tokens} \footnote{Um token é uma instância de uma sequência de caracteres.} de cada performance. Nós descobrimos que apenas uma pequena quantidade desses tokens (variando de 20 a 30\% em cada performance) podia ser encontrado no dicionário que utilizamos para comparação\footnote{Utilizamos o Hunspell Dictionary, disponível em: \url{http://hunspell.github.io/}.}. A partir desta constatação inicial, prosseguimos para uma análise temática das mensagens para uma compreensão melhor do conteúdo das mesmas.


%As all the performances were held in Brazil, the collected textual data is in Portuguese. We first conducted a computational analysis of the messages using natural language processing techniques in Python \footnote{http://www.nltk.org/}

%Table \ref{tab:words} reports the number of tokens\footnote{A token is an instance of a sequence of characters.} in each performance. We found that only a small amount of these tokens (from 20\% to 30\%) could be found in the Hunspell\footnote{http://hunspell.github.io/} dictionary. In order to obtain a more comprehensive analysis of the messages we further conducted a thematic analysis (see Section \ref{sec:thematic}).

\begin{table}[ht!]
\tabcolsep5.1pt
\caption{Words Usage}{%
\begin{tabular}{@{}lcccc@{}}
\hline
Performances & 1 & 2 & 3 & 4 \\
\hline
Número total de tokens&     2836&   1615&  1764&   1494\\
Número de tokens únicos& 1274& 1110& 960& 822\\
Tokens no dicionário &      404&   240&  292&   358\\
\% dos tokens no dicionário &       31\%&   21\%&  30\%&   43\%\\
\end{tabular}}
% \begin{tabnote}
% Words Usage on the performances.
% \end{tabnote}
\label{tab:words}
\end{table}

Utilizando o texto total das mensagens, geramos uma nuvem de tags com a ferramenta online Word Cloud \cite{JasonDavies}. Fizemos nuvens de tags individuais para cada performance, mostradas na figura \ref{fig:clouds} eu uma geral, mostrada na figura \ref{fig:cloud}. A nuvem geral apresenta alguns termos inesperados, como por exemplo a tag HTML \texttt{</marquee>}, que é utilizada para mover elementos em uma interface web. Isto foi utilizado pelos estudantes de ciência da computação na terceira performance analisada, após um dos participantes descobrir que era possível a inserção de código HTML e CSS no chat. A audiência então passou a \emph{hackear} o chat produzindo formas geométricas em tempo real\footnote{A possibilidade de inserção de códigos HTML e Javascript foi suprimida nas versões posteriores do software}. outras palavras notadas foram nomes ou apelidos de pessoas presentes na audiência como ``DJ", ``Johnny" e ``Maestro", palavras com cunho político como ``Fora'' e o caractere de coração. 


%We generated a tag cloud of tokens using the Word Cloud online tool~\cite{JasonDavies}. The tag cloud, displayed in Figure \ref{fig:cloud}, shows some unexpected terms, such as the HTML \texttt{</marquee>} which is used to move elements in a web interface. The process was used by the Computer Science students in the third performance at IME after one of them realized that it was possible to insert CSS and HTML code in the chat. The audience then started to hack the chat by producing shapes in a live coding fashion\footnote{We later modified the application to avoid the possibility to hack the interface in this way -- how to use these HTML functions in a meaningful way could however be the object of further investigations}. Other unexpected terms recurring frequently include the heart character, the expression ``Fora (Out)", which has a clear political meaning (see Section \ref{sec:thematic}), and people's nicknames, such as ``DJ", ``Johnny" and ``Maestro".

\begin{figure}[ht!]
\centering
\includegraphics[width=1\linewidth]{pictures/cap3/wordcloud_performances}
\caption{Nuvem de tags gerada pelas palavras utilizadas nas performances analisadas. (Gerada pela autora utilizando a ferramenta Word Cloud).}
\label{fig:clouds}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=1\linewidth]{pictures/cap3/wordcloud_pb}
\caption{Nuvem de tags gerada pelas palavras utilizadas nas performances analisadas. (Gerada pela autora utilizando a ferramenta Word Cloud).}
\label{fig:cloud}
\end{figure}

A análise revelou que uma grande quantidade de \emph{tokens}, eram palavras de apenas um caractere, que representavam cerca de 19 dos 30 tokens mais usados. Nós acreditamos que isso se deu pela natureza do projeto, uma vez que cada caractere representava um som, então os participantes poderiam usar desta estratégia para gerar um som específico. 

%The analysis revealed that a large number of tokens are single characters which represent 19 out of the 30 most used tokens. We believe that this relates to the specific nature of the Open Band chat application which turns any single character into sounds. This indicates that intent of participants could either focused on semantic meaning or on the resulting sonorities, which we analyzed further in the thematic analysis.

\subsubsection{Análise temática}
\label{sec:thematic}

Para investigar o conteúdo das mensagens, utilizamos uma metodologia de análise temática proposta por Braun e Clarke \cite{Braun2006}. Inicialmente fizemos uma análise da função linguística das mensagens, e posteriormente analisamos fatores mais relacionados ao seu conteúdo semântico.

%We analyzed the message log data using a thematic analysis following the methodology proposed by Braun and Clarke~\cite{Braun2006}. The analysis was conducted at two levels, one looking at the function of the messages, the other looking more closely at the semantic content.

%\subsubsection{Functional analysis}

Inicialmente, identificamos dois grandes grupos de mensagens, um com mensagens com conteúdo semântico explícito, e outro cujas mensagens não pareciam ter conteúdo semântico. As mensagens do primeiro grupo foram divididas nos seguintes grupos:

%We first identified two major groups of messages, depending on whether they included explicit semantic content or if their content was not semantical. The semantic messages were categorized according to their functions, as follows:

\begin{description}
\item[Fática] -- mensagens utilizadas com a principal função de testagem do canal, como:``Oi", ``Olá", etc.
\item[Metelinguística] -- mensagens que referenciavam o projeto ou a performance propriamente dita, como comentários e elogios, por exemplo.
\item[Denominante] -- mensagens que identificavam indivíduos, que podiam ser membros da audiência ou celebridades.
\item[Política] -- mensagens com conteúdo político.
\item[Onomatopaica] -- mensagens com onomatopéias em português ou palavras representando sons.
\item[Risadas] -- mensagens com expressões de risada, um tipo específico de onomatopéia, que foi separado do grupo pela quantidade significante de mensagens. 
\item[Declarações] -- declarações ou afirmações em geral; esta categoria inclui um conteúdo discursivo mais variado.
\item[Código] -- mensagens com códigos de programação ou comandos para mudar os sons.
\item[Emoticons] -- mensagens com emoticons pictóricos, que não eram mapeados em sons, e mensagens formadas por emoticons desenhados por caracteres.
\end{description}

As mensagens de conteúdo não semântico foram divididas em três subgrupos:

\begin{description}
\item[Letras únicas] -- mensagens consistindo de apenas um caractere.
\item[Padrões] -- mensagens que incluíam repetições de caracteres ou de sequências de letras, como um loop de letras.
\item[Aleatórios] -- sequências de caracteres sem lógica.
\end{description}

\begin{figure}[ht!]
\centering
\includegraphics[width=1\linewidth]{pictures/cap3/bar_plot_new_revised}
\caption{Gráfico mostrando as proporções dos temas recorrentes nas performances.}
\label{donut}
\end{figure}

O gráfico representado na Figura \ref{donut} mostra a proporção de mensagens em cada tema identificado na média das performances. De uma maneira geral, podemos notar que a maior quantidade de mensagens em todas performances foram as mensagens de caracteres sozinhos. Nós pudemos também notar que as audiências tiveram comportamentos diferentes com relação à proporção de temas durante as performances. Por exemplo, na performance p1, os caracteres sozinhos foram mais explorados, enquanto na p3 foram menos, mensagens de conteúdo de código foram mais utilizadas na p3, onde a audiência era de cientistas da computação.

\begin{figure}[ht!]
\centering
\includegraphics[width=1\linewidth]{pictures/cap3/subj_themes}
\caption{Gráfico mostrando as proporções dos temas subjetivos nas performances.}
\label{subj}
\end{figure}


 %the proportion of the messages in each of the identified themes considering all the performances on average. Overall we can see that messages containing a single character is the most dominating theme. We can note that the audience had different behaviors in respect of the proportion of themes used during the performances. For example, much more exploring "single character'' messages in p1 and less in p3, and much more use of "code'' in p3.  

Depois de categorizar as mensagens em temas de acordo com suas funções linguísticas, fizemos uma nova análise temática, desta vez procurando buscar mais especificamente mensagens de suporte ou crítica, e também com a tentativa de perceber diferenças de comportamentos entre os diferentes conjuntos de samples. A figura \ref{subj} mostra a distribuição de temas subjetivos pelas performances. Nós identificamos os seguintes temas\footnote{Os temas não são exclsivos, podendo ser aplicados a mais de uma menagem, número total de mensagens entre parêntesis}: sem sentido (1989), slogans (180), questões (73), bullying (77), imperativo (99), auto-presença (81), reclamações (89), amor (84), humor (284), música (174), satisfação (518), e teste/condução (237). Exemplos incluem slogans (e.g. ``Do it"), amor (e.g. ``Paz e amor"), expressões de auto-presença (e.g.``Estou online"), referencias a outros músicos (``Lady gaga") e programas de TV (``Winter is coming" referência à série Game of Thrones), questões (``tem alguém aí?"). 

%After categorizing the messages into functional themes, we further analyzed their content to search for differences in expressions of support from the audience and how the sample packs affected the nature of the messages. We identified  the following (non exclusive) themes (total number of messages reported in brackets): senseless (1989), slogan (180), questioning (73), bullying (77), commanding (99), self-presence (81), complaining (89), love (84), humor (284), music (174), satisfaction (518), and testing/conduction (237). Examples include slogans (e.g. ``Do it"), love (e.g. ``Paz e amor": `` Peace and love"), expressions of self-presence (e.g.``I'm online"), references to musicians (``Lady gaga") and TV shows (``Winter is coming" referring to the Game of Thrones series), questions (``Is there anybody out there?"). %Some out of context copy/pasted text also appeared on the logs now and then.

\begin{table}[ht!]
\tabcolsep8.1pt
\caption{Tabela de frequência de mensagens em cada performance (p) e cada pacote de samples (sp)}{
\begin{tabular}{ c|c|c|c|c|c  }
        & \multicolumn{4}{ c| }{Performance} \\ 
  Pacote de samples & p1 & p2 & p3 & p4& TOTAL\\ \hline     
  sp0   &613 & 430 & 126 & 385 & 1154\\
  sp1   &20 & 81 & 39 & 135 & 275\\
  sp2   &142 & 82 & 143 & 64 & 431\\
  sp3   &783 & 256 & 286 & 139& 1464 \\ \hline
  TOTAL &1558& 849 & 594 & 723& 3724\\
\end{tabular}}
% \begin{tabnote}
% Crosstable of Message Occurrence Per Sample Pack (sp) and Performances (p).
% \end{tabnote}
\label{tbl:perf_sample_xtab}
\end{table}

%\subsection{Link Between Message Content and Sample Pack}

Para investigar como os conjuntos de samples afetaram a natureza e o conteúdo das mensagens enviadas pelos participantes, separamos as mensagens enviadas em cada conjunto de samples (ver Tabela \ref{tbl:perf_sample_xtab}). No total, foram 3724 mensagens enviadas no total pelo sistema. 

%In order to investigate how the various sample packs affect the nature and content of messages sent by participants, we broke down the amount of unique sent messages according to the sample pack (sp) to which they were associated (see Table~\ref{tbl:perf_sample_xtab}). In total, there were 3724 written messages sent during all four performances with various proportions for each sample pack. 


\begin{figure}
\centering
\includegraphics[width=1.1\linewidth]{pictures/cap3/p_values}
\caption{Frequências de conteúdos por pacote de sample.}
\label{subj_themes}
\end{figure}

A Tabela \ref{subj_themes} apresenta a frequência dos temas entre os conjuntos de samples. Usamos análise estatística para checar se haviam diferenças entre a frequência dos temas de acordo com a mudança de conjunto de samples durante as performances. Como a quantidade de mensagens e o tempo de cada conjunto de samples era variável entre as apresentações, comparamos as frequências dos temas utilizando um teste qui-quadrado de independência. Como referência, utilizamos testes estatísticos descritos em \cite{beasley1995multiple} e \cite{garcia2003cellwise}. 

%Table \ref{subj_themes} presents the content theme frequencies across sample packs. Several differences can be noted and their significance was tested using statistical analyses. A Chi-square test of independence was calculated to compare the theme frequencies across sample packs. A significant interaction effect was found ($\chi^2$ (33) = 181.97, $p < .00001$). We performed the post-hoc test described in \cite{beasley1995multiple} and \cite{garcia2003cellwise} using Bonferroni adjusted alpha levels of 0.001.

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{pictures/cap3/p_values}
\caption{Variação na frequência de temas entre pacotes de samples. O tamanho dos círculos representa o tamanho do resíduo padrão (quanto maior o círculo, maior a diferença de frequência). A gradação de cor representa diminuição (vermelho) e ampliação (azul).}
\label{fig:bblplot2}
\end{figure}

A Figura \ref{fig:bblplot2} mostra as variações entre frequências de temas entre pacotes de samples, expressa pelo tamanho do resíduo padrão. As diferenças mais significativas foram encontradas entre os conjuntos sp0 e sp2. no caso do conjunto 0 (galáxias), mensagens sem sentido foram mais frequentes do que nos demais samples, e mensagens de amor menos. Isto talvez possa ser explicado pelo fato de que os sons mais estranhos estivessem nos caracteres especiais e numerais, fazendo com que os participantes exploram efeitos sonoros utilizando esses caracteres, que criaram um contraste com os demais sons vocais. Quanto ao sp1 (percussão e acordes), não houve diferença significante, isso pode ser porque ele foi usado menos tempo durante todas as performances, por desejo dos próprios condutores, pois os sons eram mais repetitivos e tradicionais. O conjunto sp2 (colaborativo) apresentou mais mensagens de amor e menos mensagens sem sentido, o que pode ser devido ao fato de não haver uma correlação esperada entre sons e letras. Isso pode ter feito com que os participantes usassem mais discurso formal ao invés de experimentarem com padrões de ritmo. 

De fato, os conjuntos sp0 (Galáxias), sp1 (Percussão e Acordes) e sp3 (Orquestra Errante) foram produzidos por um método mais rigoroso de tradução intersemiótica, e foram os que levaram a mais experimentação fora do campo do discurso nas performances analisadas. 

%Figure~\ref{fig:bblplot2} displays the sources of variations of theme frequency across sample packs expressed as the size of the standardized residual. The post hoc analysis showed that significant differences of theme frequency occurred for Sample Packs 0 and 2, and for Sample Pack 3, to a lesser extent. With sp0 differences were obtained for the themes 'Love', 'Senseless' and 'Testing/Conductor' ($p < .001$). As Figure ~\ref{fig:bblplot2} shows, 'Love'-themed messages were less frequent for sp0 while 'Senseless'-themed messages occurred more frequently. This may be explained by the fact that for this sample pack, special characters were  mapped to sound effects creating unexpected contrasts compared to letters that were associated to vocal sounds. Participants may have preferred to use the special characters for these reasons leading to a more frequent occurrence of messages without semantic meaning (senseless).

%No significant changes were found for sp1. This may be because this sample pack was on overall less used for musical purposes, given that its sounds are repetitive and conductors preferred to use the pack for introduction to the system and short durations. Contrary to sp0, for sp2 'Love'-themed messages were used more frequently and there were less 'Senseless' messages in this sample pack. This may come from the fact that for this sound pack there is no expected association between letters and sounds. Participants may hence have employed a more discursive style instead of exploring letter to sound association in a rhythmic way like for other sample packs. Indeed the sample packs sp0, sp1 and sp3 which were composed using a rigorous inter-semiotic translation approach (see Section ) lead more consistently to non-semantic expressions in the chat system, contrasting with sp2 which includes sounds composed collaboratively. For sp3, the only significant difference was related to the 'Testing/Conductor' theme. As sp3 was used mostly at the end of the pieces, this is connected to the commands entered by conductors to direct the closure of the performances.

Não houve nenhuma variação significativa na frequência dos temas ``satisfação'', ``humor'' e ``reclamações'' entre os pacotes de samples, o que pode demonstrar que o grau de satisfação não foi significativamente alterado ou pelo menos não foi registrado pelos participantes de uma maneira geral. 

%There weren't any significant theme frequency variations for 'Satisfaction', 'Humor' and 'Complaint' messages between sample packs. This may indicate that the choice of sample packs did not influence the overall hedonic experience of the participants.



%\subsection{Design}

%Various text-to-sound mappings were investigated throughout the development of Open Band: by (i) mapping letters to sounds using sample-based synthesis ~\cite{Stolfi2017} following a procedure of inter-semiotic translation~\cite{JulioPlaza1969}, or by (ii) using web audio synthesis following an isomorphic relation between the sound spectrum and the letters~\cite{Stolfi2017b}. In both cases, the mappings between the letters and the sounds were established a priori by the author and experimenters, so the only creative sonic control for the audience consisted in proposing combinations of sounds over time. Although analyses of data collected during performances indicated that the experience was fun and engaging for the audience~\cite{Stolfi2018}, we found that the capabilities of the system to support creative musical composition were limited due to the constraints and limited sonic agency. 


\subsubsection{Conclusões}


Um dos nossos objetivos era de prover uma plataforma para livre expressão da audiência, como uma ``web ágora''. Como atestar isso de uma maneira científica foi uma questão que norteou o processo de análise que envolveu processos quantitativos (de análise estatística) e qualitativos (análise temática). A diversidade de temas que surgiram nas análises indicam que este objetivo foi atingido, uma vez que os participantes expressaram diversas posições e discutiram temas que variaram de amor a opiniões políticas divergentes, expondo também preconceitos e \emph{nonsense}.

A análise temática das mensagens, revelou um grande grau de suporte dos usuários, que pudemos medir pela quantidade significante de mensagens e satisfação (cerca de 14\% de todas mensagens). A quantidade de mensagens também indica um grande engajamento do público, com mais de mil mensagens enviadas a cada performance (ver tabela \ref{tab:msgtype}). Esses dados indicam que a plataforma foi acessada com facilidade e acessibilidade, mesmo em casos onde a audiência não tinha necessariamente conhecimentos musicais prévios. 

%One of our design goals was to provide a platform for free audience expression as a web ``agora". The various themes which emerged from the analyses endorse this idea as participants felt free to discuss subjects ranging from love to political opinions.

%The thematic analysis revealed a high degree of appreciation and support from participants; the second most frequently occurring theme was 'Satisfaction' with 518 messages across performances (14\% of all messages). Log data analyses revealed that participants actively engaged in the performances as over a thousand messages were played in each performance (see Table \ref{tab:msgtype}). Such behavior and the expressions of support in the chat communication are signs that the tool was easily accessible by audience members and that no previous musical knowledge is necessary to engage with it.

Ficou claro que a interface de chat encorajou os participantes a usar a plataforma como um meio de comunicação para conversarem entre si, mas apesar das semelhanças que o sistema pode ter com outros sistemas de comunicação online, quase metade das mensagens escritas não tinham significado semântico. Essas mensagens, que dividimos em três subcategorias (caracteres sozinhos, padrões e aleatórias) indicam que a audiência também utilizou o sistema para explorar o sistema de forma musical, explorando ritmos e sons de maneira estética.

%The two categories of semantic and non-semantic messages appear in almost equal proportions across performances. It is clear that the chat interface encouraged participants to use Open Band as a way to communicate with each other. Screen projection of the chat made Open Band a compelling means for posting messages to be shared with ones's entourage, as is done in other social media platforms. However, although Open Band's interface shares similarity with other web-based platforms for verbal communication, approximately half of the messages did not show any explicit semantic content. These messages, which can be divided into three sub-categories (single characters, patterns, and random-like) point to the exploration of the musical potential of Open Band, since they served to engage with the system's sonic features and to performatively explore these features.

Tocar repetidamente a mesma letra pode ser visto como uma forma de testar o sistema, mas também como forma de disparar sons em um ritmo específico, e também de ouvir aquele som único no meio da massa sonora composta pelas frases. é similar à função fática no contexto semântico, como aponta Jakobson que serve principalmente para estabelecer, prolongar ou encerrar a comunicação \cite{Jakobson}. Também permite com que os usuários reconheçam a sua contribuição específica dentro da performance e reconhecer os sons individuais. Padrões, por outro lado, ou repetições de sequências curtas permitem aos participantes a criação de motivos rítmicos ou frases musicais, que também se destacam do conteúdo textual que em geral não tem muita repetição, se aproximando da linguagem musical.

%Repeatedly typing the same letter can be seen as a way to test the system and check its communicative capabilities. It is similar to the phatic function in the semantic context ``primarily serving to establish, to prolong, or to discontinue communication" \cite{Jakobson}. By repeating a same keyboard key the user could recognize the relationship between individual letters and the sounds they produce. In addition, repetition allows a clear identification of each participant's own sonic contribution within the somewhat complex flow of sounds that can be produced during a performance. On the other hand, the loop repetition of short sequences allows for the creation of rhythmic motifs that stand out from the random sonorities produced at certain moments. Finally, texts with non-semantic content produced by the almost random triggering of the keys are strong indicators of participation in performing Open Band as a music practice.

Como aponta uma das mais importantes referências da cultura moderna brasileira nas artes, o Manifesto Antropofágo de Oswald de Andrade de 1928 \cite{Andrade1928}, ``a alegria é a prova dos nove''. Nesse sentido, os dados recolhidos, assim como a observação do comportamento da audiência apontam uma evidência de que o projeto foi bem sucedido nesse sentido, com várias referências de humor, declarações de apoio e risadas registradas pela audiência. 

%Possibly one of the biggest influences in Brazilian modern culture, the 1928 ``Cannibal Manifest" from poet Oswald de Andrade~\cite{Andrade1928} presents an important standpoint for our work, ``Joy is the real proof". From this standpoint the data provide some evidence that the performances were experienced in a playful way, as there were many statements and declarations of fun and enjoyment in participant messages.

Sob outro ponto de vista, que era o de propor uma ``Obra Aberta'', nós consideramos que o sistema poderia ser mais aberto, uma vez que os usuários tinham liberdade somente para escolher o texto, mas não para fugir dos sons pré-programados pela compositora e escolhidos pelo condutor durante as performances. Em se tratando de uma audiência que não tinha necessariamente treinamento musical, essa opção foi importante para manter a consistência estética do projeto.

%From another standpoint, which was to propose an Open Work, we still consider the current system insufficient, as users have little freedom about the sounds to use since these are restricted to the ones chosen by the composer and the conductors during the performance. We maintained the control of sound production mechanisms fairly simple in favor of ease of use and intuitiveness. With such an approach the system is open to audience without prior musical knowledge, thus minimizing the amount of instructions required to participate. To maintain artistic consistency of live performances, the sample packs' choices were left to the artists facilitating the performances. %The choice of constraints was also decided to maintain the artistic consistency of the live performances in musical terms.



%However to increase the agency of participants, users should be able to map themselves the sound into letters, or at least be able to decide between broader sound options. To attempt to address this issue, we are developing as part of the Audio Commons\footnote{\url{http://audiocommons.org}} project, a new platform using the Freesound Application Programming Interface~\cite{Font2013}. The prototype, Playsound.space, is a web-based interface (Figure ~\ref{playsound}) that let users search sounds in the Freesound database using semantic queries. Future work will investigate how letters can be mapped into sounds in the Playsound platform. Combining the Open Band and Playsound frameworks may lead to new interesting applications for the re-purposing of crowd-sourced content for music making and instant messaging auditory display.


%We wanted to expand it to be a framework to be used in different musical contexts. One of our goals is to integrate the project with the audio Commons API\cite{Akkermans2011} to foster media re-purposing, creating an interface for uploading and mapping sounds to the letters. For the web audio synthesis version, we aim to develop form to control the base frequencies and rhythm trough gestural interfaces, to enhance the musical possibilities given to the audience.

Apesar do projeto Banda Aberta ter se mostrado interessante como experiência participativa e projeto de performances, os resultados sonoros e as possibilidades criativas de produção musical ainda não estavam suficientes para suprir necessidades pessoais individuais. Como performance participativa, foi interessante porque demonstrou que o texto podia ser uma forma fácil e intuitiva de interação musical. Considero que a experiência do projeto Banda Aberta foi interessante como possibilidade de explorar uma composição musical interativa, e também de explorar um processo compositivo de montagem sonora coletiva, mas com um nível de constrição muito alto. 

A experiência foi bastante importante também para demonstrar as capacidades de processamento, controle e acessibilidade das tecnologias web para práticas musicais. Em nenhuma das apresentações realizadas, sob diversas condições de aparelhagem e sistemas de som, consideramos que a qualidade sonora fosse suficiente. O sistema também se mostrou muito versátil e adaptável à mobilidade e condições diversas de aparelhagem de som. Em um caso extremo, no festival Áudio Insurgência, a mesa de som da casa estava com defeito, e tivemos que ligar o sistema direto do celular com um cabo p10/RCA.

Como um instrumento para performance ao vivo, no entanto, ainda me parecia insuficiente. Durante minhas atividades práticas musicais, tive a oportunidade de tocar com o sistema em performances de improvisação livre em algumas situações, como na apresentação para o festival aMostra Sonora. Durante a performance, onde utilizei também um patch de Pure Data que desenvolvo há cerca de 10 anos, percebi que a quantidade de sons não era suficiente para proporcionar uma variação sonora satisfatória em performances longas. O sistema, que é propositalmente simples, para que seja o mais leve possível, não permite processamentos mais complexos do som, e por mais que o conjunto de sons fosse grande, a falta de controle em fatores mais sutis, que são importantes em uma performance solo de improvisação musical me deixaram entediada com o sistema durante a apresentação. Para isso, precisaria também de algo que realmente servisse como um instrumento musical, e não somente como um mapeamento fixo. 

A partir daí, surgiu a idéia de um projeto que envolvesse a API do Freesound, que inicialmente pensava como um framework que expandisse as potencialidades sonoras deste projeto. A idéia inicial foi o de usar a API do Freesound como um método para mapear os sons em letras, que iriam ser acessadas dentro do sistema do Banda Aberta. Para conseguir desenvolver essa tecnologia, escrevi um projeto para um estágio de pesquisa junto ao Centre for Digital Music (C4DM) da Queen Mary University of London (QMUL),  sob orientação do professor Mathieu Barthet, uma vez que lá havia um dos grupos de trabalho do projeto Audio Commons, que entre outras coisas, estava dando suporte ao Freesound. 

\begin{otherlanguage*}{brazil}

\section{Playsound.space}

O desenvolvimento da plataforma Playsound.space (PS) começou durante o meu período de estágio no Centre for Digital Music (C4DM) na Queen Mary University of London (QMUL)\footnote{O Estágio aconteceu de junho de 2017 a maio de 2018 e foi financiado pelo Programa de Doutorado Sanduíche da CAPES}, onde tive a oportunidade de participar do grupo de pesquisa ligado ao projeto Audio Commons\cite{Font2016}. Depois de desenvolver o projeto Banda Aberta, o desejo era de trabalhar no desenvolvimento de um sistema que pudesse ser utilizado como um instrumento musical, que fosse capaz de produzir uma gama rica de sonoridades e não mais somente uma plataforma para tocar sons pré determinados. 

A iniciativa Audio Commons visa trazer conteúdo sonoro em Creative Commons (CC) para artistas e indústrias criativas. Licenças CC fornecem uma maneira padronizada para dar permissão ao público no compartilhamento e utilização de trabalho criativo em condições definidas pelos criadores de conteúdo, que pesquisa formas de aproveitamento e utilização de serviços online de distribuição de conteúdo sonoro com licenças em Creative Commons footnote{\url{https://creativecommons.org/}}. O projeto é financiado pela união Européia e tem entre seus objetivos, desenvolver uma ontologia para sons, e criar um mecanismo mediador para pesquisar sons de diversas fontes como as bibliotecas Freesound.org, um grande repositório de samples; Europeana.org, que reúne um acervo de gravações históricas de diversas instituições européias e Jamendo.com, que reúne músicas novas produzidas em licenças livres.


Nosso principal domínio de aplicação é a improvisação musical que é definida como uma atividade musical autônoma \cite{Canonne2016} que geralmente leva a situações pluralistas, com ênfase no processo de tocar, e na iteração musical no momento \cite{BERGSTROEM-NIELSEN2016}. Em oposição à improvisação idiomática, como aquela praticada em algumas formas de jazz ou hip-hop, a improvisação livre pode levar à formas não metrificadas e sem escala ou tonalidade pré-determinadas, onde muitas vezes a variação de timbre prevalece\cite{Barthet:11a}. Já vinha desenvolvendo atividades em improvisação livre anteriormente, mas depois do início do doutorado, tive oportunidade de participar da Orquestra Errante, grupo conduzido pelo professor Rogério Costa que ensaia semanalmente no estúdio do NuSom na Universidade de São Paulo. 

Durante as práticas de improvisação musical que participei até agora, encontrava algumas dificuldades em utilizar softwares tradicionais como DAW ou \emph{patchers}. Uma delas é de que muitos softwares do tipo DAW são baseados em grids temporais fixos, ou seja, existe um tempo que determina o fluxo dos acontecimentos sonoros, e embora esse tempo possa ser mudado, a estrutura rígida conflita com a necessidade da liberdade na improvisação. A estrutura em grade ou se impõe para os demais músicos, como um metrônomo, ou entra em conflito com os demais participantes. Além disso, as estruturas temporais também dificultam a criação de polirritmias. 

Softwares que se comportam como instrumentos virtuais, por outro lado, como sintetizadores e \emph{samplers} são mais fáceis de serem empregados na prática. Por serem baseados em gesto, o controle do fluxo sonoro fica a cargo do musicista, dependendo aí do tipo de controlador que ele usa, de sua expertise técnica em tocar, e da capacidade de variação timbrística do instrumento. No caso dos sintetizadores, as possibilidades de variação de sonoridade são constritas pelos timbres oferecidos pelo fabricante ou programador, e em geral restritas a sons musicais, dentro de uma escala pré-determinada. Além disso, para se obter um bom controle de dinâmica, é recomendado a utilização de controladores externos, como teclados MIDI, por exemplo. Minha idéia era desenvolver algo que pudesse ser tocado em tempo real, e que permitisse mais variação sonora do que os softwares e ferramentas disponíveis no mercado.

No contexto de novas interfaces para produção musical uma série de abordagens diferente têm sido desenvolvidas para o emprego do computador como instrumento musical na prática de improvisação livre. Exemplos incluem \emph{live coding} \cite{freeman2011collaborative} e orquestras de laptop \cite{Albert2012}. \emph{Live Coding} colaborativo ao vivo frequentemente envolve o desenvolvimento de tecnologia para sincronização entre dispositivos \cite{Wilson2014}, que aqui não foi adotada devido à escolha estética de deixar a estrutura rítmica livre.

A ideia de tocar com uma ``paleta de sons expandida'' tem sido explorada na música desde Luigi Russolo  \cite{Merz2013} e especialmente depois da música concreta. A digitalização e a disponibilização de sons online potencializa essa ideia, como aponta Schnell:
\begin{citacao}
``In the age of digital sound databases and online music publishing services, the total disembodiment of digital sound turns into the promise of perpetual reincarnation of digital sounds through their permanent exchange and transformation."\cite{Schnell2013}
\end{citacao}

Nos instrumentos que funcionam a base de amostras de sons (samplers), as potencialidades sonoras são ampliadas pela possibilidade de utilização de sons não-musicais, ou em outras escalas, mas são dependes de se ter acesso e conhecimento de uma biblioteca grande de sons. Localizar samples em tempo real durante uma improvisação musical pode ser desafiador\cite{Xambo2018}, principalmente porque a improvisação exige do musicista uma reação espontânea e instantânea em tempo real \cite{canonne2011model}. Isso exige que o performer conheça bem e previamente os sons de uma determinada coleção, o que se torna impraticável se a coleção de sons é muito grande. Para contornar este problema, os praticantes normalmente selecionam uma amostra reduzida de sons, o que acaba também por reduzir suas possibilidades criativas durante as performances.

A digitalização do som, em conjunto com tecnologias Web e bancos de dados de áudio digital abre muitas possibilidades criativas, que como Schnell aponta, pode levar à ``promessa de reencarnação perpétua de sons digitais através da sua permanente troca e transformação'' \cite{Schnell2013}. A utilização de amostras de sons pré-gravados é largamente empregada em uma série de tradições estéticas musicais como no emph{Hip Hop, Plunderphonics, Música Eletrônica, Música Concreta, composição de Paisagens Sonoras}. Bibliotecas online de áudio como Freesound.org, Redpanal.org, Sampleswap.org entre outras são utilizadas por compositores e produtores musicais de vários tipos de aplicações multimídia como cinema, publicidade, video games, e composições musicais \cite{Roma2013}. 

Alguns projetos desenvolvidos recentemente têm também esse norte como paradigma. O projeto API Cultor, por exemplo \cite{Ordiales2017} usa técnicas de \emph{machine learning} para prover um ambiente para re-utilização de sons de blibliotecas online. Lee et al. propõe uma ferramenta para \emph{live coding} com a API do Youtube para improvisação livre \cite{Lee}. Ao prover acesso a seu banco de dados por uma REST API \cite{Akkermans2011}, o site Freesound.org permite que musicistas e designers criem aplicativos que explorem seu conteúdo online para utilização ao vivo. BeatPush \cite{Feenstra2016}, é um exemplo de sequenciador usando esta API e o Freesound Explorer \cite{Font2016}, por exemplo, organiza os sons em uma configuração espacial por similaridade e usa cores para representar aspectos timbrais, no entanto, é uma aplicação mais voltada para navegação e exploração do que para tocar em tempo real, e não permite que os usuários selecionem sons a partir de buscas múltiplas. 


Entre diversos serviços que provém conteúdo sonoro online, uma imensa gama de sons musicais e não musicais são oferecidos pelo Audio Commons Ecossystem \cite{Font2015}. A ideia no desenvolvimento do Playsound era de ser uma tentativa de contornar essas questões, promovendo o acesso a esses sons em tempo real através da API do Freesound \cite{Akkermans2011}, oferecendo feedback visual através de espectrográficos, de uma forma que pudesse ser tocada sem um grid de tempo fixo e por usuários sem domínio de técnicas musicais.


Arne Eingenfeld - Metacration Lab
Coming Together - Freesound
url{https://www.youtube.com/watch?v=jFD2A8bX8TM}

An autonomous soundscape composition created by four autonomous artificial agents. Agents choose sounds from a large pre- analyzed database of soundscape recordings (from freesound.org), based upon their spectral content and metadata tags. Agents analyze, in realtime, other agent's audio, and attempt to avoid dominant spectral areas of other agents, selecting sounds that do not mask one another. Furthermore, selections from the database are constrained by metadata tags describing the sounds. Thus, water sounds may trigger other water sounds, or agents can choose to oppose contextual references. As the composition progresses, convergence is further facilitated by lowering the bandwidth of the agent's resonant filters, projecting an artificial harmonic field upon the recordings that are derived from the spectral content of the recordings themselves. Finally, each agent adds granulated instrumental tones at the resonant frequencies, thereby completing the ``coming together''. \cite{Arneeigenfeldt2010}

Freesound Radio


Compor a partir de espectrogramas era uma idéia que acompanhava meu trabalho já faz algum tempo. Em 2011 publiquei um trabalho chamado UTOPIA (Figura \ref{utopia}), onde desenhava a palavra utopia através de síntese subtrativa sobre uma gravação feita de uma serra de fita em funcionamento, que era uma amostra bastante saturada. Essa idéia também voltou outras vezes no meu trabalho, na composição da peça Bandas Críticas e no processo de composição de sons para o Banda Aberta. Quando começamos a publicar os primeiros artigos a respeito do projeto Banda Aberta comecei a buscar ferramentas para conseguir imprimir os conjuntos de samples (ver figuras \ref{samplesgalaxias}, \ref{samplespercussao}, \ref{samplescolab} e \ref{samplesorquestra}) e não consegui encontrar nenhuma ferramenta pronta que pudesse gerar espectrogramas de um conjunto grande de sons que fosse acessível, então para gerar essas imagens, bem como os sites que reúnem os samples do projeto, precisamos desenvolver uma ferramenta própria, que chamamos de spectrogram player, que foi o esboço de um player para tocar a partir de espectrogramas, em JavaScript e HTML \footnote{A ferramenta foi desenvolvida em código aberto e está disponível no endereço: \url{https://github.com/arianestolfi/spectrogramplayer}}. Quando comecei a desenvolver este novo projeto, descobri que a API do Freesound já fornecia os espectrogramas dos sons de sua biblioteca, o que era muito conveniente para o projeto, já que diminui o tempo necessário para a análise via FFT que poderia gerar os espectrogramas em tempo real. Além disso, ao oferecer os espectrogramas como imagens, a API do Freesound permite realizar a pesquisa sonora sem a necessidade de baixar os sons toda vez no computador do usuário.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/utopia}
\caption{\label{pstimeline}Utopia }
\label{utopia}
\end{figure}

Queria desenvolver uma ferramenta que não dependesse de expertise técnica ou virtuosismo, que é um dos objetivos dessa pesquisa. Assim como no projeto Banda Aberta, decidimos manter o texto como forma de interação com o sistema, mas ao invés de fazer um mapeamento de sons por letras, como no projeto anterior, aqui o texto serve como fonte para buscar informações, ao permitir a busca através de significados semânticos ou descritivos, por exemplo: ``chuva pacífica'', ``crowd noise'' ou ``applause''. A solução técnica foi o desenvolvimento de um sistema de busca e de um tocador que permite o acesso a centenas de milhares de sons em Creative Commons baseada na API do Freesound.

\subsection{Motivações}




\subsection{Desenvolvimento do Projeto}




Comecei a desenvolver o projeto em Julho de 2018, após apresentar o Banda Aberta em alguns eventos na Europa que descrevi na seção anterior. Estava acompanhando semanalmente as reuniões do projeto AC e imaginei um sistema onde pudesse se buscar os sons através de busca de texto, e escolhê-los pelo espectrograma. A primeira idéia, no entanto, era fazer uma espécie de linha do tempo onde se poderia arrastar os sons e compor no espaço da tela. Quando comecei a desenvolver, ideia de linha do tempo foi substituida pela playlist, para que o sistema não ficasse constrito a uma janela de loop fixo. 

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/firstsketch}
\caption{\label{firstsketch}Primeiro rascunho do sistema, que ainda se chamava audioquery}
\label{fig:firstsketch}
\end{figure}


A figura \ref{fig:timeline} apresenta os principais estágios de desenvolvimento da ferramenta de Setembro de 2017 a Julho de 2018. Utilizei novamente Lean Ux \cite{Liikkanen2014} como metodologia de desenvolvimento de software. Dentro dos princípios desse método, começamos novamente o projeto a partir de um protótipo bem simples, que era apenas um sistema de busca que mostrava o resultado como um conjunto de espectrogramas. Inicialmente, contei com a ajuda do programador Miguel Ceriani para fazer a ligação com a API do freesound.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/playsoundtimeline}
\caption{\label{pstimeline}Playsound development timeline}
\label{fig:timeline}
\end{figure}

Utilizamos como \emph{framework} Angular.js\footnote{Angular.js é um \emph{framework} em JavaScript desenvolvido pela Google que permite automatizar certos processos computacionais e facilita a comunicação com bancos de dados}. O Framework fornece o recurso de ligação de dados bidirecional, que faz com que a busca aconteça no servidor simultaneamente ao se digitar o texto na caixa de busca. Deste modo, mesmo antes de se completar uma palavra, resultados já começam a aparecer na janela do navegador. Para o processo de improvisação livre, esse recurso se mostrou muito interessante, uma vez que sons não esperados podem surgir mesmo antes de se estabelecer um vocábulo definitivo. O site foi construído como um aplicativo de página única, o que permite com que os conteúdos sejam alterados sem que haja um recarregamento da página\cite{Jadhav}. Assim, a interação dos usuários não interrompe o fluxo sonoro.

Os resultados são apresentados na forma de espectrogramas, que permitem que o usuário do sistema tenha informações sobre ritmo e timbre das amostras recebidas antes de escolher o som para tocar. Aparecem como uma matriz, que permite que se compare os sons visualmente. Apesar de a leitura dos espectrogramas não ser uma coisa corriqueira para qualquer usuário do sistema, acreditamos que um aprendizado implícito pode acontecer no simples processo de pesquisar e tocar com o sistema, quando se percebe a co-relação entre a representação gráfica das propriedades espectro-temporais dos dos sons e suas qualidades audíveis. Quando selecionamos uma imagem, o som é adicionado a uma playlist na lateral da interface.

 Assim que colocamos o sistema no ar, começamos a desenvolver recursos adicionais para transformar o sistema em um instrumento musical de fato. O primeiro recurso desenvolvido foi a capacidade de se fazer novas buscas enquanto os sons são tocados, recurso que já não existe no próprio Freesound. Em seguida, criamos um sistema de url para armazenar uma coleção de sons feita previamente. Cada som selecionado gera um código que fica registrado no endereço do navegador. Desta forma, é possível recuperar uma ``composição de sons'' para utilização futura. O próximo passo foi desenvolver a interface para tocar os arquivos. A primeira versão funcionava baseada em objetos HTML, utilizando o \emph{player} padrão dos navegadores para objetos de áudio que oferece controles apenas de pausar tocar, alterar o instante tocado e dependendo do navegador um controle de volume. Em uma segunda versão utilizamos o tocador do Freesound, que oferecia recursos de loop, mas isso exigia que se recarregasse a página, interrompendo o fluxo musical. 

 Desenvolvemos alguns recursos básicos do tocador, adicionando a imagem do espectro sonoro como recurso mnemônico, e adicionamos controle individual de volume e loop para cada som que era adicionado à playlist. Como recurso de usabilidade, para dar feedback visual, a transparência da imagem é alterada conforme o volume do som aumenta ou abaixa. Adicionamos também um gravador embutido no sistema, que permite que as seções sejam gravadas em arquivos WAV. Esses arquivos podem ser salvos ou re-inseridos na interface para serem tocados. A figura \ref{fig:audioquery} mostra a interface da primeira versão do software no Google Chrome, que até esse momento chamávamos de Audioquery. \footnote{Por questões acadêmicas, mantemos ainda uma versão funcional do software em \url{http://www.codigo.xyz/audioquery/\#/sounds=49333,415849}}.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/audioquery_browsers}
\caption{\label{audiquery}Primeira versão funcional do software desenvolvida.}
\label{fig:audioquery}
\end{figure}

\subsection{Primeiras avaliações}

Durante esta primeira fase, as avaliações foram feitas principalmente pela autora, mas também contando com a opinião de alguns colegas convidados para testar informalmente a ferramenta. Esses testes fora de condições de laboratório são previstos dentro de uma perspectiva de metodologias ágeis de programação. A partir desses primeiros testes, pudemos observar informalmente o comportamento de alguns usuários com o sistema e afinar o sistema para realizar testes de usuário mais formais. 

A primeira peça gravada com o sistema chamei de ``spacefrogs''\footnote{Disponível em: https://soundcloud.com/asss/spacefrogs}, foi publicada em novembro de 2017. Foi uma sessão de improviso feita com temática espacial e pântanosas, como qe criando uma atmosfera de ficção científica. Naquele momento, o sistema ainda não contava com controles de volume individuais para cada faixa. Podemos notar pela análise espectral da peça (Ver figura \ref{fig:spacefrogs} e \ref{fig:spacefrogsdt} ) que existe um grande contraste de materiais sonoros, mas pouca variação de dinâmica nos materiais empregados.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/spacefrogs_spectrogram}
\caption{Espectrogramas da faixa ``spacefrogs''. Gerado pela autora utilizando o software sonic visualiser.}
\label{fig:spacefrogs}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/spacefrogs_spectrogram_dt}
\caption{Detalhes da faixa ``spacefrogs''. Gerado pela autora utilizando o software sonic visualiser.}
\label{fig:spacefrogsdt}
\end{figure}



%Alguns resultados desses testes informais foram gravados, como quando apresentei o sistema para o designer de som 


 %Mais adiante, passei a contar com a ajuda da {}programadora Alessia Millo, que colaborou no desenvolvimento do tocador e de outros recursos que implantamos no sistema. Ela trabalhou na adaptação do sistema para utilizar as tecnologias de Web Audio, ao invés dos objetos HTML, que permitiu uma série de recursos que implementamos posteriormente, como a possibilidade de escolher o começo e o fim dos pontos de loop, alterar a velocidade de reprodução de sons e posição estéreo dos sons.

\subsubsection{Puppets Ensemble}

A partir do ponto que consideramos o sistema satisfatório para ser utilizado em performances ao vivo, (\ref{fig:timeline}), passamos a realizar com ele processos de avaliação mais formais. 

O primeiro deles, foi um processo de avaliação baseado na prática, e para isso, queríamos testar o sistema em condições ``reais'' de uso. Como a principal função do sistema era o de ser empregado em práticas de improvisação livre, montamos um pequeno grupo de improvisação na QMUL, o ``Puppets Ensemble'' para o qual convidamos músicos e interessados dentro do C4DM. Marcamos alguns ensaios, com músicos tocando com o sistema e outros tocando instrumentos tradicionais ou eletrônicos que já praticavam. A proposta desses ensaios, que duraram cerca de uma hora cada e aconteceram no Laboratório de Performance da QMUL, era de tocar algumas sessões de improvisação livre de cerca de 10 minutos e discutir, na sequência de cada sessão, as impressões estéticas e o uso da ferramenta no processo.

Nessas sessões, toquei com Playsound e também com um microfone onde improvisei com a voz. Além disso, participaram Anna Xambó (Supercollider e Playsound), Simin Yang (Playsound), Parham Bahadoran (Percussão e aplicativos de celular). Luca Truchet (Playsound) e Mathieu Barthet (guitarra com efeitos), três mulheres e três homens, com idade média de 33 anos (Ver arranjo na tabela \ref{tab:puppets}). Dos participantes, somente Chen não tinha prática anterior em improvisação musical. Mesmo assim, ela foi capaz de utilizar o sistema sem treinamento anterior.

Com esse processo, pudemos testar se o sistema poderia ser usado de fato como um instrumento musical, e se era possível de ser utilizado em conjunto com outros instrumentos em situações reais de performance. Em todas sessões, tanto quem tocou com a ferramenta quanto os demais musicistas ficaram satisfeitos com as improvisações. Nenhum dos participantes tinham tocado juntos previamente e foi possível estabelecer um diálogo musical durante as sessões. A ferramenta foi elogiada pela riqueza dos sons que provia, Um dos participantes comentou que gostava do fato de que ``qualquer ideia de som que eu tenho eu posso ter nas minhas mãos''. 

O feedback dos usuários também foi importante para notarmos algumas limitações do sistema naquele momento. O primeiro protótipo, não tinha por exemplo o nome dos sons na playlist, o que dificultava o uso para pessoas sem prática de leitura dos espectrogramas. Também não tinha controle individual de volume para cada som, então a possibilidade de controle de dinâmica ainda era muito reduzida, e incluímos melhorias nesse sentido para realizar as próximas rodadas de avaliação.   \footnote{Gravações das seções podem ser encontradas em \url{http://finetanks.com/records/puppets/}}.

\begin{table}
\centering
\caption{Performers in music improvisation mixed ensemble sessions: (M): musician; (N): non-musician.}
\begin{tabular}{|lc|} \hline
Sessões & Performers \\ \hline
1 & Ariane (M), Anna (M), Simin (N), Mathieu (M)\\ \hline
2 & Ariane (M), Parham (M), Mathieu (M) \\ \hline
3 & Ariane (M), Simin (N), P4 (M), Mathieu (M), Luca (M)\\
\hline\end{tabular}
\label{tab:puppets}
\end{table}

\subsubsection{Teste com usuários}

Depois desta primeira avaliação prática, passamos a discutir como faríamos uma avaliação mais formal e se isso seria necessário para a publicação de um primeiro artigo científico sobre o uso da ferramenta, que foi publicado na Conferência New Interfaces for Music Expression (NIME) de 2018. Isso gerou um grande debate com meu orientador na QMUL, uma vez que testes de usuário em laboratório poderiam ser contradizentes com a ideia geral norteadora do projeto, que era o desenvolver um software sobretudo para uso pessoal e não necessariamente um produto para o mercado. 


A contradição maior era de que esses testes demorariam muito tempo e a análise dos resultados seria muito complexa, tomando tempo que poderia ser empregado no desenvolvimento e programação do sistema. Por outro lado, uma avaliação formal ou seja, ``que apresente um roteiro estruturado de coleta de dados e resultados'' \cite{Stowell} era importante para o processo, e também para conseguir comparar a ferramenta com outras que estavam sendo desenvolvidas dentro do contexto do projeto Audio Commons, Audio texture, Sample Surfer e o próprio Freesound, para tanto, aplicaríamos um questionário similar ao que já estava sendo utilizado para avaliar essas ferramentas.


Decidimos então fazer o teste mais formal, em condições controladas de laboratório, para o qual convidamos 15 voluntários, com graus diferentes de intimidade com práticas musicais e de improvisação livre (Figura \ref{usertest}). O pesquisador Luca Truchet se dispôs a colaborar na aplicação dos testes e na análise estatística dos resultados obtidas, assim como meu supervisor no C4DM, que fez a análise temática das respostas dos questionários. Nesse momento também, onde começamos a divulgar a ferramenta para o público, passamos a chamar o sistema de Playsound e registramos o domínio onde o site está hospedado.


\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/usertest}
\caption{\label{usertest}Fotos dos testes com usuários no laboratório do C4DM.}
\label{fig:SUS}
\end{figure}
%When the system was judged ready to be used in live performances (Audioquery multitrack player in Figure \ref{fig:timeline}), we conducted several user evaluations \cite{Stolfi2018b}. First, we established a small ensemble including musicians playing PS and other musicians playing electronic or traditional instruments such as guitar and effects, piano, percussions, and vocals. In each session, participants were invited to play together an improvisation piece for about 10 minutes, and to discuss their experience after each piece. We examined three sessions with this kind of ensemble, involving 6 musicians in total. Following this study, we improved the interface to provide individual volume control and information about the audio file on the playlist.\footnote{Audio recordings of seven 10 min improvisation pieces are available at the link:\url{http://finetanks.com/records/puppets/}}


Existe muita discussão sobre como fazer testes de avaliação de ferramentas para performance musical ao vivo \cite{Barbosa2015}, como aponta Stowell \cite{Stowell}, porque ``interações musicais tem aspectos criativos e afetivos'', que não são fáceis de medir através de tarefas, como é comum no campo do design de interação. Além disso, existe um debate sobre abordagens qualitativas, que são baseadas em relatos sobre a experiência e quantitativas, baseadas em estatísticas e levantamentos quantitativos. Nós decidimos por uma abordagem mista, com aspectos qualitativos e quantitativos.

Como atividade, propusemos aos voluntários, que foram organizados em cinco trios. Inicialmente, os participantes foram brevemente instruídos sobre o funcionamento do sistema, e em seguida convidados para tocar três sessões de improvisação livre. Como o grau de envolvimento dos participantes com esse tipo de prática musical era diferente, dissemos que poderiam tocar o que quisessem, mas que tentassem ouvir o que os outros estavam tocando para compor uma peça coletiva. Após cada sessão de improvisação, discutimos brevemente os resultados estéticos e como a ferramenta influenciou nesses resultados. O final, aplicamos um questionário (Ver Anexo), que incluíam questões sobre o perfil demográfico dos participantes (idade, gênero e experiência musical), de usabilidade (escala SUS de usabilidade \cite{Jordan1996}), questões específicas para medir o suporte à criatividade \cite{Cherry2014} e questões gerais para feedback sobre o sistema. 

As questões de usabilidade foram aplicadas utilizando uma escala Likert de cinco pontos \footnote{A escala Likert é uma escala psicométrica para medir o grau de concordância com determinadas afirmações, indo de discordo totalmente a concordo totalmente, com gradações intermediárias}. Foram incluídas também algumas questões para medir níveis de engajamento, aprendizado, inovação, relevância, qualidade da descoberta dos sons, familiaridade e utilidade dos espectrogramas. Essas questões respondidas em escala Likert foram submetidas a análises estatísticas usando o teste de Mann-Whitney-Wilcoxon, para observarmos também se havia alguma variação significativa na satisfação dos usuários músicos e não músicos.

Quanto à usabilidade e engajamento, não houve diferença significativa entre os resultados obtidos pelos músicos e pelos não-músicos. A Figura \ref{fig:SUS} mostra os resultados obtidos na avaliação de usabilidade. Em geral, os usuários definiram o sistema como fácil de usar, fácil de aprender a usar, conveniente, que não havia necessidade de aprender muita coisa antes do uso, não necessita de suporte técnico para seu uso, que não é um sistema desnecessariamente complexo. Os usuários foram neutros quanto à possibilidade de usar o sistema frequentemente, isso talvez seja relacionado com a tarefa, já que vários usuários não têm perspectivas de práticas em improvisação livre. De um modo geral, o sistema obteve uma nota muito boa seguindo a métrica do sistema SUS (Média = 82.5, Desvio Padrão = 8.94)

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/SUS_lower}
\caption{\label{amas}Resultados obtidos na avaiação de usabiliade.}
\label{fig:SUS}
\end{figure}

Também fizemos uma avaliação quantitativa das respostas dos usuários sobre níveis engajamento, aprendizado e sobre a interface. A figura \ref{fig:questionnaire}, que apresenta os resultados obtidos indica que os participantes se sentiram engajados de uma maneira geral. Quanto à questão a respeito de se os usuários haviam aprendido algo com o instrumento, as respostas foram neutras. Nessa questão as pessoas que tinha mais prática com música também foram as que discordaram mais sobre aprender algo com o software. Os participantes concordaram em média que a forma de compor música usando Playsound era novidade para eles, e tiveram diferentes graus de dificuldade para encontrar os sons que esperavam durante as sessões. 

Quanto à familiaridade com o uso de espectrogramas, tivemos também um resultado neutro, uma vez que a maioria dos participantes tinha familiaridade e o restante não. Também foi neutro o resultado a respeito da utilidade do uso dos espectrogramas, que variou de acordo com a familiaridade dos usuários.


\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/questionnaire_lower}
\caption{\label{amas}Resultados obtidos na avaiação do questionário.}
\label{fig:questionnaire}
\end{figure}

Uma segunda parte do questionário incluiu questões relativas à medição do índice de suporte à criatividade (CSI). O método \cite{Cherry2014}, descitro por Cherry inclui um questionário psicométrico que busca testar a capacidade de uma ferramenta para dar suporte aos processos criativos de seus usuários. Para isso, apresenta algumas questões a respeito do desempenho em quesitos de exploração, expressividade, imersão, colaboração, prazer, e resultados em função do esforço. Em seguida, fazemos uma comparação par a par entre seis fatores para determinar quais são os fatores mais importantes para os usuários em relação ao uso da ferramenta: ser criativo e expressivo; se tornar imerso na atividade; gostar de usar o sistema ou ferramenta; explorar várias possibilidades de ideias, resultados e possibilidades; produzir resultados que fizeram valer o esforço despendido; trabalhar com outras pessoas. Os resultados obtidos por esse método de medição também foram bastante satisfatórios, com uma média de 71.7 (Desvio padrão de 15.6), considerando o estágio de desenvolvimento do projeto e a tarefa proposta.


\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/CSI}
\caption{\label{amas}Resultados do questionário CSI, como os maiores valores médios em negrito.}
\label{fig:questionnaire}
\end{figure}


 
%In another evaluation, PS was tested in controlled lab settings with trios of participants both with and without prior music performance experience. Participants were invited to play three five-minute long free improvisations using the system on their laptops. 15 participants took part in the study (5 females, 10 males, mean$\pm$SD age = 32.7$\pm$5.4), 8 of them considered themselves as musicians (4 intermediate and 4 experienced), while 7 did not. We measured system usability \cite{Jordan1996} and creativity support \cite{Cherry2014} using an online survey to be completed just after the performances. We also conducted inductive thematic analyses \cite{Braun2006} from focus group discussions and self-reports related to workflow, hedonic quality, engagement, learning, contexts of use and improvements. The prototype yielded a high usability score (M = 82.5/100, SD = 8.94) and creativity support index (M =71.7, SD =15.6) with no significant differences between non musicians and musicians.

Nós conduzimos análises temáticas \cite{Braun2006}, tanto doas discussões realizadas após a execução das peças, quanto das respostas e comentários dos usuários apresentados nos questionários. 

Nas discussões realizadas em grupo, reconhecemos os seguintes temas:

\begin{description}
 \item[Satisfação] Sete participantes expressaram verbalmente que gostaram de tocar com outras pessoas usando o sistema

  \item[Expressividade] Três participantes expressaram um grande nível de satisfação sobre a possibilidade de tocar com qualquer tipo de conteúdo sonoro (``Eu gosto do fato de poder pegar imediatamente qualquer tipo de som que vem na minha cabeça e usar isso para compor em tempo real'')

 \item[Monitoramento] Nove participantes mencionaram a impossibilidade de se ouvir o som antes de tocar e desconforto com o fato dos sons serem tocados instantaneamente em volume alto. Alguns mencionaram estratégias para contornar esse problema, eles clicavam bem rapidamente para diminuir o volume do som assim que selecionavam os samples e em seguida faziam manualmente um processo de fade-in.

 \item[Relevância e surpresa] Cinco participantes mencionaram que os sons tocados não correspondiam totalmente as expectativas geradas pelas buscas textuais, que se relaciona com o tagueamento e descrição dos sons no próprio Freesound. Três participantes também mencionaram o fator surpresa como algo interessante para gerar novas ideias sonoras.

 \item[Controle Expressivo] Seis participantes, todos músicos, sentiram necessidade de haver mais controles de expressividade musical, uma vez que naquele momento a interface permitia somente o ajuste de volume de cada som. Eles mencionaram como sugestão um controle geral do volume da página; possibilidade de sincronização das batidas; associação de teclas do teclado com sons; possibilidade de avaliar os sons para ranquear os sons preferidos e a possibilidade de escolher os pontos de início e fim dos loops. Duas pessoas mencionaram que a impossibilidade de sincronização fez com mudassem suas estratégias composicionais (``eu evitei sons com ritmo e procurei sons não musicais'').

  \item[Identificação] Cinco pessoas mencionaram dificuldade em identificar quais sons estavam tocando ou sendo tocados por outras pessoas, um deles sugeriu construir algo colaborativo onde se pudesse ver o que os outros estavam tocando.

\item[Suporte à criatividade e narrativa] quatro participantes mencionaram que tentaram criaruma narrativa em relação ao que os outros estavam tocando, a partir da seleção das palavras-chave em suas buscas (``Eu procurei as palavras-chave para me adaptar ao contexto. Uma ideia de alguém desencadeou outra ideia em mim, então criamos uma narrativa todos juntos'', ``tentei responder ao que os outros estavam tocando, por exemplo, se ouvia ele tocando pássaros, eu procurava encontrar sons de gatos'').

\item[Utilidade dos espectrogramas] Cinco participantes, que tinham fundamentos em tecnologia musical mencionaram que os espectrogramas foram úteis para o processo (``O espectrograma realmente me ajudou a ler os sons e minhas decisões foram baseadas nisso''). Um deles comentou ``é um modo diferente de tocar: eu estou usando meus olhos para tocar música''. Outros cinco participantes, que não conseguiam ler os espectrogramas, relataram que tinham que confiar nas informações fornecidas junto com os arquivos (nome e duração) para tocar.

\end{description}

Nós também fizemos uma análise temática das respostas das questões discursivas apresentadas nos questionários. As 12 questões analisadas foram: 

\begin{itemize}

\item Descreva brevemente seu processo de trabalho usando Playsound

\item O que você mais gostou sobre o Playsound?

\item O que você menos gostou sobre o Playsound?

\item Por favor avalie seu engajamento ao utilizar Playsound para tocar com outras pessoas (Por favor explique brevemente sua escolha)

\item Eu senti que aprendi algo novo ao usar Playsound. (Por favor explique brevemente sua escolha)

\item A forma que compus música usando Playsound foi nova para mim. (Por favor explique brevemente sua escolha)

\item Ao tocar com Playsound que tipos de sons você procurou? (ex. sons musicais, instrumentos acústicos, sons não musicais, gravações de campo, efeitos sonoros, loops, sons de fala)

\item Eu consegui encontrar os sons que eu procurava. (Por favor explique brevemente sua escolha e comente sobre a relevância e qualidade dos sons encontrados)

\item Por favor descreva que melhorias você faria no sistema (ex. interface do usuário, tipos de sons, controles, etc)

\item Por favor descreva em quais contextos de uso você consideraria utilizar o Playsound.

\item Por favor indique algum outro provedor de conteúdo que você estaria interessado em acessar usando uma ferramenta como Playsound

\item Sinta se livre para adicionar qualquer comentário sobre a experiência ou o estudo.

\end{itemize}

A análise temática quantitativa, que foi conduzida por Mathieu Barthet com o auxílio do software MAXQDA\footnote{\url{https://www.maxqda.com/}}, encontrou 681 códigos diferentes nas respostas dos usuários. Os temas mais importantes foram: suporte à criatividade musical (128 ocorrências), busca sonora (64 ocorrências), limitações (61 ocorrências), engajamento emocional (56 ocorrências), técnica de tocar e agência criativa (52 ocorrências), melhorias (44 ocorrências), usabilidade (37 ocorrências) e contextos de uso (24 ocorrências). Abaixo apresentamos um resumo dos resultados encontrados:

\begin{description}
\item[Suporte à criatividade musical e busca sonora] Os usuários relataram frequentemente questões ligadas à criatividade musical, colaboração criativa, expressividade e riqueza dos sons encontrados, como``a possibilidade de tocar qualquer som que viesse à minha cabeça'', ou ``essa ferramenta me permitiu explorar um vocabulário sonoro que eu não costumo usar pra tocar''. Uma grande variedade de conteúdo sonoro foi utilizada durante as sessões, incluindo: sons musicais (ex. relativos a gêneros musicais ou instrumentos); sons temáticos, correspondentes a uma idéia ou tópico; sons de fundo (atmosferas e ambiências); sons de fala (ex. beatbox), efeitos especiais e sons sintéticos (ex. glitches, sons processados espectralmente, sons granulares); sons rítmicos (como padrões e loops de bateria, batidas, etc) e sons relativos à natureza (ex. ``pássaros'').

\item[Limitações] As limitações apresentadas pelos usuários sobre o sistema incluíam questões relacionadas à barreiras criativas e questões técnicas. Quanto às barreiras criativas, houveram menções ao fato de não ser possível reconhecer o que estava sendo tocado, sobre a curva de aprendizado, aleatoriedade dos resultados, falta de controle, falta de feedback sobre as atitudes dos outros. As questões técnicas foram relativas à falta de controle, falta de monitoramento, para saber o que seria tocado de antemão, controle do volume, relevância dos sons, metadados e qualidade sonora (ex. variação de dinâmica e equalização de volume)

\item[Engajamento emocional e estratégias de tocar] Um grande número de ocorrências (56) mencionam um engajamento emocional positivo com o sistema e as tarefas. Frequentemente mencionaram satisfação (18), engajamento (15), diversão (8), interesse (6), fluxo (5) e imersão (3). Muitas ocorrências (52) descrevem uma série de técnicas ou estratégias empregadas pelos participantes. Elas incluem tocar por idéias semânticas, liderar a composição, tocar ritmicamente, usar loops, tentar procurar sons semelhantes, tocar através de ideias musicais, sobreposição de camadas de sons, ser inspirado pelos outros, usar descoberta generativa, adicionar elementos faltantes ou divertidos, tocar randomicamente, por tentativa e erro, etc. Essa lista longa de estratégias possíveis foi um bom indicativo de que o sistema era útil como suporte para criatividade musical. 

\item[Melhorias] O participantes indicaram várias melhorias possíveis para o sistema (44 ocorrências). Muitos aspectos relativos aos processos de controle sonoro, como melhor controle do volume e do loop, velocidade do sample, efeitos especiais, filtros e interface para controladores externos. Muitos mencionaram a falta de um monitor para ouvir os sons em um fone de ouvido antes de tocar, recurso que exigiria o uso de uma interface de áudio externa. Sugeriram também processos para sincronizar, ou agrupar sons, busca por parâmetros sonoros, como BPM ou gênero, por exemplo. Outro ponto comentado foi quanto a percepção do que se está tocando e do que os outros estavam tocando. Também foram mencionados aspectos de controle de dinâmica como fades automáticos, controle geral de volume e compressão de áudio. Outras fontes de conteúdo sonoro possíveis mencionadas foram Youtube e RedPanal.

\item[Usabilidade] Aspectos de usabilidade foram notados. A simplicidade do sistema e clareza da interface foi notada por alguns participantes. Um deles apontou que ``é muito intuitivo''. Quanto ao uso dos espectrogramas, houveram menções em sentidos opostos. Alguns acharam útil enquanto outros não, fato que está relacionado com a experiência dos usuários nesse tipo de representação. Alguns usuários solicitaram mais informações para auxiliar a escolha dos sons. Muitos apontaram também positivamente a velocidade do sistema eu facilidade para encontrar os sons.

\item[Contextos de uso] Vários contextos possíveis de uso foram apresentados pelos participantes, como: improvisação livre; pesquisa de sons e samples; trilha sonora para vídeos; improvisações de dança; orquestras de laptop; como ferramenta de rascunho para produções musicais posteriores; composição de paisagens sonoras ou para se divertir.

Apesar de consumir um grande esforço, a realização dos testes com usuários foi uma experiência muito produtiva para o desenvolvimento do sistema. A possibilidade de obter feedback de pessoas de fora do projeto, e de testar como o sistema poderia ser utilizado por outras pessoas foi muito rica e os comentários observados foram bastante significativos. Para os usuários com mais experiência em música, a simplicidade e a não constrição a uma estrutura de grade rítmica foi notada com certo desconforto, enquanto os usuários sem experiência prática musical puderam usar o sistema com mais facilidade, todo, no entanto, foram capazes de tocar em conjunto e produziram resultados musicais interessantes. Isso foi de encontro à proposta inicial do projeto de criar uma ferramenta intuitiva direcionada a pessoas sem domínio de técnicas musicais, e gerou ideias para o desenvolvimento do projeto. O material gravado durante as sessões também serviu para produzir um pequeno vídeo demonstrativo do sistema \footnote{Disponível em: \url{https://youtu.be/yv8T70rawzs}}, que foi apresentado na reunião do projeto Audio Commons em Luxembrugo no mês de fevereiro de 2018. 



\subsubsection{Avaliação prática}

Pouco tempo após a realização dessa avaliação mais formal em laboratório, tive a oportunidade de testar o sistema na prática em uma performance solo, no evento A'mas que aconteceu no Total Refreshment Centre em Londres, no dia 25 de Março de 2018\footnote{Um trecho da performance pode ser assistido em: \url{https://youtu.be/LmjmpQagBG8}}. Para a ocasião, decidi trabalhar com um material pré-selecionado, para garantir uma consistência durante a performance. Depois de uma performance solo de 30 minutos, onde toquei com o sistema e pratiquei improvisação vocal. Decidi por fazer uma seleção de bases, que usei para criar texturas e variações de ritmos durante a performance. 

No final, também participei de uma jam session com os outros 8 músicos. durante a jam, uma parte dos músicos estava conectada através de um relógio MIDI central, que sincronizava o BPM dos sistemas baseados em grid, como Playsound não é baseado em grid, tive que desenvolver estratégias específicas para não entrar em conflito com a grade musical que se estabeleceu. Isso incluiu tocar com texturas mais longas e não rítmicas, tocar com o botão play no ritmo (dispensando o recurso de loop), saturar elementos curtos em alguns momentos.

Apesar de ter sido possível tocar, houveram alguns problemas que notei durante a performance. Quando empregamos sons pré-selecionados, tivemos alguns problemas relativos a performance do computador devido a um sobrecarregamento da mémória. Além disso, senti o grau de controle de processos sonoros poderia ser desenvolvido para melhor aproveitamento dos materiais sonoros durante a performance.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/ariane_amas}
\caption{\label{amas}Performance solo no evento A'mas em Londres.}
\label{fig:amas}
\end{figure}


\end{description}





%These initial user evaluations were followed by a first live solo performance with PS by the first author at the A'mas event held at the Total Refreshment Centre in London on 25 March 2017 \footnote{Excerpt from the performance can be found at \url{https://youtu.be/LmjmpQagBG8}}. After a 30 minutes solo performance, the performer also joined a jam session with 8 other invited musicians who played synthesizers and other electronic instruments. During the jam, most of the instruments were connected through a central Midi clock providing beat synchronization. Although PS does not offer this possibility, since it is not grid-based, it was still possible to play live in this format. The performer had to develop a strategy to select in real time sounds that wouldn't conflict with the established rhythm, working with sonic materials such as textures and effects instead of more structured loops. By using this system as part of a public live electronic performance, we acknowledged that it could be used as a basis for real music practice. But as in previous evaluations, the performer also found that the creative control would benefit from enriched audio processing.



%``Non musician, Sophie''
%I got more and more into it the longer I played around with it and I think that the longer we all played together, the more coherent outcomes we produced. despite the difficulties of not always knowing to what extend I contributed what sounds, it was still enjoyable to listen to the atmosphere we managed to create.


%I tried to listen to other musicians and react to their playing. Some times I instead took the initiative to propose my ideas, even if they were not coherent with the narrative proposed by the others. This was to create something new, to give another direction to the music


\subsection{Playsound WebAudio}

Em seguida a esse primeiro círculo de desenvolvimento e avaliações, meu desejo era o de trabalhar no desensenvolvimento do tocador de áudio. Naquele momento, estava trabalhando como professora auxiliar no curso de ``Creative Coding'' (programação criativa), que estava sendo ministrado por Alessia Milo na Universidade de Greenwich. O curso abordava tópicos como o uso de tecnologias web e API's em projetos artísticos, afins a
 esta pesquisa. Ela se interessou em colaborar com o desenvolvimento do projeto e implementar tecnologias que permitissem um maior controle e processamento dos sons tocados.

 A partir das análises das experiências realizadas, organizamos uma lista de demandas prioritárias para trabalhar, também observamos que algumas demandas apontadas pelos usuários, por exemplo, iam de encontro com necessidades que também senti como performer, enquanto outras eram divergentes dos objetivos gerais do projeto.

 A questão da sincronização de batidas, por exemplo, que alguns usuários mencionaram (todos eles músicos experientes), não é prioritária, uma vez que já existem várias opções de software profissionais para atender essas demandas, e um dos princípios norteadores desse trabalho era de propor algo que pudesse ser tocado sem esse tipo de constrição. Outro ponto apontado por alguns usuários seria a criação de um sistema para monitoramento dos samples antes de tocar. Para que isso seja possível, é necessário o uso de uma interface de áudio externa, o que diminui também a acessibilidade do site. Gostaríamos de manter o foco da pesquisa justamente em desenvolver soluções que não exijam a instalação de nenhum outro equipamento para tocar, além do computador ou celular com acesso à internet. 

 Uma questão que abordamos foi o fato dos sons sempre tocarem instantaneamente com a seleção, em volume médio. Alguns voluntários apontaram essa questão, que também senti na prática durante a performance solo. Para contornar essa questão, mudamos a forma com que os sons são tocados. Na nova versão, ao clicar na imagem correspondente ao som, o som é colocado na playlist, mas ele não toca, então é possível selecionar o volume desejado antes de tocar o sample. Adicionamos também um pequeno botãozinho de play sobre a imagem, que permite ainda tocar instantaneamente o som desejado.

 Desenvolvemos também a interface, para que fosse possível selecionar um trecho do loop para tocar. Para implementar esse recurso, mudamos toda a base do processamento de áudio do sistema. Enquanto a primeira versão se apoiava nos elementos HTML para tocar, na nova versão passamos a empregar a WebAudio API de forma mais direta, através do elemento \emph{buffer}, que carrega os sons na página e os manipula diretamente através de JavaScript. Implementamos também \emph{panning}, para distribuir os sons nos canais estéreo, e o controle da velocidade de reprodução do sample (\emph{playbackrate}), que permite mudar também o tom do sample (alterando também a duração em conjunto. Nas figuras \ref{handbell} e \ref{handbelldt}, que apresentam espectrogramas da peça da autora``Handbell'' \footnote{Disponível em: \url{https://soundcloud.com/asss/handbell}}, feita a partir da manipulação de um sample usando PS, podemos notar as capacidades transformativas dos novos recursos adicionados. 



\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/handbellreverse}
\caption{\label{handbell}Espectrograma da peça Handbellreverse.}
\label{fig:handbell}
\end{figure}




%\subsection{Melhorias}

%Following users' desire for more expressive control, we added a range of audio editing, processing and mixing capabilities. This included the possibility of queuing sounds in the playlist and manipulating their duration and pitch by varying their playback speed. We also enabled editing by selecting segments and control custom loops during playback. To implement this feature, we used the buffer object from the Web Audio API library which replaced the HTML media element object (despite the HTML media object can start playing upon selection large files while still buffering). We also introduced a panning control for each sound object, allowing to position them in the stereo field. We are planning to introduce a panning control on the master channel e.g. to help musicians from laptop ensembles to identify individual sonic contributions spatially.



\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/handbellreverse_detail}
\caption{\label{handbelldt}Detalhe do espectrograma da peça.}
\label{fig:handbelldt}
\end{figure}


Uma mudança importante da primeira versnao para a segunda, que na verdade não é uma melhoria mas uma restriçnao, foi motivada por discussões éticas quanto à questão dos direitos autorais e licensas dos sons. Enquanto na primeira versão oferecíamos acesso a todos os sons do banco de dados do Freesound, independente da licensa. Decidimos restringir o acesso somente aos sons com licensas Creative Commons 0 (que equivale ao domínio público) e Attribtion, que exige a citação do autor dos sons originais. Essa decisão, que excluiu uma quantidade relativamente pequena de sons\footnote{Em um levantamento em novembro de 2018, haviam 395183 sons no total no banco de dados do Freesound, sendo que deles 47361 tinham licensa atribuição não comercial.}, mas muitos deles de boa qualidade foi motivada não só por uma questão de evitar possíveis problemas legais decorridos do uso do software, como também por uma questão ética, de incentivar o uso de licensas livres por parte dos usuários do Playsound. Dessa forma, se um músico quiser utilizar seus próprios sons na plataforma\footnote{Como fizemos por exemplo na performance Tender Buttons | Sound | Space, que está descrita mais adiante nesta tese}, deverá necessariamente publicá-los com uma licensa menos restritiva. 

Em conjunto com a implementação deste filtro, colocamos também na interface do site uma seção de créditos, que se atualiza automaticamente na medida em que sons com a licensa de ``Atribuição'' são tocados. Desta forma é fácil para alguém que queira fazer uso dos sons gerados pela ferramenta de dar os devidos créditos aos produtores de conteúdo original. 

Na segunda versão, colocamos também em cada player, um link para a postagem do som original no Freesound, para quem quiser baixar o áudio em alta qualidade para fazer uso em algum outro programa de edição, por exemplo. 



\subsubsection{Teste com a Orquestra Errante}
Os testes com usuários feitos em laboratório foram úteis para comparar o sistema com outros projetos que também estavam sendo desenvolvidos no contexto do projeto Audio Commons, mas apesar disso, estávamos mais interessados em avaliar como o instrumento se sairia em condições mais realistas de uso. Para isso, seria interessante testar o projeto com grupos musicais já estabelecidos. Ao término do meu estágio de pesquisa na QMUL e antes de assumir a vaga de professora na Universidade Federal do Sul da Bahia, tive a oportunidade de passar alguns dias em São Paulo, onde consegui marcar uma sessão de avaliação do PS com a Orquestra Errante, grupo coordenado pelo professor Rogério Costa, do qual também faço parte. Avaliando criticamente o processo de avaliação em laboratório na QMUL, decidi concentrar os esforços na avaliação prática do instrumento, e otimizar o tempo dos músicos no ensaio, deixando o preenchimento do questionário como opcional para quem quisesse responder fora do horário do ensaio. No final, apenas 3 dos oito participantes preencheram o questionário, o que inviabilizou as análises quantitativas

A dinâmica do trabalho do grupo é não hierarquizada, onde ``a criação se dá sempre de forma colaborativa, coletiva, compartilhada em tempo real e irrepetível.'' \cite{costa2013orquestra}. O grupo sempre discute as propostas musicais apresentadas e como elas devem ser executadas pelo coletivo dos músicos. Sendo assim, deixamos também para discutir com os músicos no dia do ensaio os procedimentos que seriam seguidos e os arranjos para organizar os grupos para testar o sistema para a prática de improvisação livre. Nós decidimos conjuntamente organizar o grupo para tocar três peças curtas (de cerca de 2 minutos) em trios, com um usuário do sistema e dois músicos tocando seus instrumentos tradicionais e três peças mais longas (de tempo livre) com dois músicos tocando PS e os demais tocando seus instrumentos conjuntamente (Figura \ref{psorquestra}). Em cada sessão foram alternados os usuários do sistema de modo que todos membros da Orquestra puderam experimentar com a ferramenta. Antes de tocar, os músicos tinha um período curto de familiarização com o instrumento que levou cerca de 3 a 8 minutos, dependendo do músico. Depois de cada sessão, discutimos coletivamente sobre o instrumento e a sonoridade resultante de seu emprego no processo de improvisação.


Participaram do ensaio os músicos: Miguel D. Antar (contrabaixo acústico), Marina Mapurunga (violino com efeitos), Fábio Martinez (saxofone e piano), Fábio Manzone (percussão), Rogério Costa (saxofone), Caio Rigui (flauta) e Stênio Biazoni, e Yonara Dantas, que não se considera musicista, mas participou também do ensaio tocando a ferramenta. A tabela \ref{tab:orchestra} mostra a formação e a duração de cada uma das sessões tocadas\footnote{A sessões foram gravadas e o registro pode ser escutado em: \url{http://finetanks.com/records/playsound/orquestraerrante}}.  

\begin{table}[ht]
\caption{Configuração de instrumentos e duração das peças durante o ensaio com a Orquestra Errante.}
\begin{tabular}{ll}
\textbf{Instrumentos}                                                    & \textbf{Duração} \\
PS + contrabaixo + violino                                & 2:34"         \\
PS + percussão + voz                                                 & 3:40"         \\
PS + flauta + sax                                                        & 2:30"         \\
PS + PS + percussão + voz + sax + flauta + piano                      & 8:04"         \\
PS + PS + sax + voz + contrabaixo + flauta + piano                   & 7:03"         \\
PS + PS + percussão + voz + contrabaixo + violino + flauta + sax & 14:51"       
\end{tabular}
\label{tab:orchestra}
\end{table}



\begin{figure}
\centering
\includegraphics[width=1\textwidth]{pictures/cap4/orquestra_errante_usertest}
\caption{\label{psorquestra}Avaliação com a Orquestra Errante.}
\label{fig:psorquestra}
\end{figure}

\subsubsection{Análise das performances}

Ouvindo as gravações do ensaio, percebemos que comparando com o teste anterior, os membros da Orquestra preferiram trabalhar com menos materiais sonoros e passar mais tempo explorando o processamento do som, ao invés de buscando sons novos. Isto pode ser devido à implantação de novos recursos para processamento de áudio, que permitem trabalhar desdobramentos musicais de um mesmo som com mais ricos do que na versão anterior, onde só havia controle de loop e volume, mas também pode ser relacionado com a experiência dos músicos em improvisação musical, que os levou a trabalhar mais no campo da sonoridade e exploração desses aspectos mais sutis de variação sonora. As peças apresentadas possuem um grau elevado de variação de dinâmica e texturas, dependendo da configuração dos grupos. Houveram interessantes diálogos musicais entre os musicistas, com diversas situações do tipo ``pergunta e resposta'' e diferentes relações de figura e fundo entre o PS e os instrumentos tradicionais. Algumas vezes, os músicos tocando PS produziram texturas onde outros músicos puderam improvisar em conjunto, em outras, agiram como solistas se destacando do contexto geral, como é de praxe nos ensaios regulares e apresentações públicas da Orquestra. 



%Audition of the recordings showed that, comparing to the previous tests, most performers from the improvisation ensemble chose to work with fewer sonic materials and spend more time at exploring sound processing instead of using large amount of sound samples. This may be due to the richer amount of expressive controls present in the updated interface, but also a will to closely work with sonorities, which is characteristic of FMI. The performed pieces present a great degree of variation in dynamics and textures, depending on the group configurations. There were interesting musical dialogues between the performers, with question-response situations and different ground-figure relations between PS and traditional instruments. At times, PS players produced accompanying textures and in other occasions, they acted as soloists, as commonly takes place in the practice of the ensemble.


\subsubsection{Análise temática}
Para conduzir a análise temática\cite{Braun2006}, transcrevi os diálogos das discussões com o grupo e as três respostas dadas nas questões discursivas, que foram versões traduzidas das mesmas aplicadas nos testes na QMUL. Nós identificamos em uma tabela em Excel, os temas seguintes, que também estavam presentes na análise da versão anterior do projeto\cite{Stolfi2018b}.

%We conducted a thematic analysis \cite{Braun2006} on the transcriptions of the group discussion and answers to the survey. We identified the recurrent themes below which were also present in the previous analysis \cite{Stolfi2018b}.

\textbf{Suporte à criatividade e narrativa.} (10 ocorrências) Usuários comentaram sobre o processo de tocar juntos (\textit{``o som que você tocou influenciou diretamente o que eu ia fazer, o contrário também aconteceu''}, \textit{``sim, eu mudei a velocidade em função do que você tocou"}), and sobre o tipo de sons tocados (\textit{``alguns sons interessantes fora de contexto que nós abraçamos, mas que foi quase no limiar do riso"} ). 

\textbf{Relevância e surpresa.} (7 ocorrências) Alguns participantes demonstraram excitação ao tocarem com a ferramenta (\textit{``a gente pode brincar?"}, , \textit{tem todos os sons do universo} \textit{``sonoramente está bem resolvido"}).

\textbf{Engajamento emocional e estratégias para tocar} (10 ocorrências) Participantes relataram a novidade da ferramenta que permitia uma forma diferente de tocar (\textit{``isso gera uma forma de tocar específica"}, \textit{``são amostras interessantes que você pode manipular como um objeto sonoro"}, \textit{``a sacada é a busca por palavras"}), e sobre como eles usaram a ferramenta durante as performances (\textit{``eu fiquei tocando, depois eu mudei o tempo, e depois usei pra selecionar os trechos de loop"}).

\textbf{Limitations.} (9 ocorrências) Os usuários relataram questões relativas à interface no momento, que deixava confusa a qual áudio os controles se relacionavam, e sobre a falta de feedback visual de qual arquivo estava tocando. Durante o estudo, a interface apresentava um \emph{bug} no elemento que indicava a posição de cada sample tocado. Os participantes também reclamaram da velocidade de resposta, uma vez que a internet estava bem lenta no estúdio e a versão atual, que funciona através de buffers, exige que se descarregue todo arquivo antes que se possa tocá-lo.


%Users reported some issues with the current interface, that had misleading controllers and lack of visual feedback about the sounds being played. During the study, the interface faced a bug preventing the playhead positions to show the current position in each audio sample. Participants also commented on the speed of the response, since the Internet speed was very slow during the test.

\textbf{Identificação de sons e fontes sonoras.} (3 ocorrências)  Usuários reportaram que foram capazes de identificar os sons digitais. Um dos participantes relatou dificuldade em reconhecer que estava tocando o quê e outro sugeriu que a performance seria melhor se cada participante tivesse seu próprio conjunto de caixas de som.

\textbf{Melhorias.} (12 ocorrências) Os participantes sugeriram melhorias desejáveis como: incluir a possibilidade de mudar a duração do som (sem alterar o pitch), sincronização de todos os sons, controle para parar todos os sons ao mesmo tempo, incluir suporte para controladores MIDI, opções de fade-in e fade-out para cada som, e interface com outros tipos de hardware, como Arduino e Raspberry Pi. 

Na resposta do questionário sobre o que os participantes mais gostaram a respeito do PS, eles mencionaram a quantidade de sons disponíveis e empoderamento (\textit{``senti uma espécie de poder por ter disponível uma imensa quantidade de sons para usar"}), e sobre a possibilidade de combinação de sons \textit{``As texturas com várias camadas sobrepostas''}. Sobre o tipo de sons pesquisados, os usuários reportaram a busca por  \textit{``sons não musicais"} e \textit{``sons graves, da natureza"}, curiosamente, um terceiro afirmou: \textit{``não procurei nada. Os sons vieram de encontro a mim."}.

\subsubsection{Chat}
Um desejo que esteve presente desde as primeiras ideias para esse software era de desenvolver uma plataforma colaborativa para performances coletivas. Quando fizemos a primeira avaliação, no laboratório da QMUL, os participantes da experiência também relataram desejos de se estabelecerem formas de comunicação entre os participantes.

Começamos então a desenvolver uma interface para um sistema de chat cujo protótipo já está em funcionamento. Entrando no endereço \url{http://www.playsound.space/chat} o usuário é direcionado para uma sala de bate papo onde por enquanto é possível se compartilhar somente texto\footnote{Em uma próxima versão do projeto, pretendemos que seja possível compartilhar também o som gerado por todos participantes}. Cada palavra na janela do chat também se torna um link conectado ao campo de busca. Com isso é possível compartilhar informação entre os usuários conectados ao sistema.

Na versão atual, o chat é baseado na tecnologia de comunicação via websockets, que foi implementada na versão atual em node.js através do uso da API socket.io. Atualmente, estamos em processo de desenvolvimento de uma nova versão do software, que permitirá aos usuários a criação de salas individuais de bate papo e compartilhamento dos processos sonoros.
%The chat is based on socket communication, implemented in node.js through socket.io. Ids are assigned automatically to the clients accessing the address, and we are currently implementing the creation of user-defined chatrooms.





%Since the last evaluation, we have included new features which will be the object of future evaluations. These features have been developed mostly to enhance participatory processes, by providing a chat environment, and accessibility, through a built-in translation system aiming to let non-English speakers use the tool.

%At the address \url{http://www.playsound.space/chat}, the user can type messages that are shared with other users connected to the same address. Moreover, the messages, once sent, become hyperlinks which, when selected, trigger a query in the Freesound database. This in return shows results for the related word. 


\subsection{Tradução}

Outra questão, que ficou ainda mais clara quando voltei ao Brasil foi de que o idioma seria uma barreira significativa para a acessibilidade do sistema. A busca na API do Freesound, é feita por um mecanismo que simples que funciona através de buscas textuais no banco de dados do site, que é construído majoritariamente em inglês. Para falantes de outra línguas, o acesso ao conteúdo do site passa por essa barreira da linguagem. Como tínhamos como objetivo desenvolver um sistema que também pudesse ser utilizado em aulas e oficinas para o estudantes no Brasil, o que nos motivou a implementar um sistema de tradução interno, que por enquanto só funciona dentro do chat, que nos permite selecionar a língua de entrada e a língua de saída, incluindo inglês, que é a língua principal nos metadados do Freesound. Esse sistema foi implementado pelo engenheiro de computação Fábio Viola, que colaborou durante alguns meses como pesquisador pos-doc no projeto Audio Commons na QMUL. Ele também começou a desenvolver um sistema de recomendações semânticas\cite{Viola2018}, que sugeria conteúdos relacionados de outros provedores como Jamendo e Europeana. Infelizmente, o desenvolvimento foi interrompido antes que fosse possível chegar em um protótipo funcional que pudesse ser empregado em performances ao vivo.

O sistema de tradução implementado é baseado na API Yandex\footnote{\url{https://tech.yandex.com/translate/}}, que suporta 90 línguas diferentes. No entanto, para facilitar a seleção, reduzimos a possibilidade às 17 línguas mais presentes no banco de dados do Freesound. Quando se digita uma palavra no campo de busca, a API sugere uma tradução, que pode ser selecionada pelo usuário para uma nova busca na língua desejada.

Para exemplificar a utilidade da ferramenta de tradução, podemos testar um exemplo, baseado na palavra em português ``pandeiro'' (``tambourine'' em inglês). No momento que implementamos a ferramenta, o Freesound tinha como resultado, 52 sons que continham a palavra no nome, descrição ou tags, se traduzirmos para o inglês, conseguimos 460 resultados com a mesma busca. mar = 274 sea = 2471
serra = 394
saw = 3662

%Freesound APIs \footnote{https://www.freesound.org/docs/api/overview.html} propose a text search mechanism that operates by matching tags and other metadata. This simple mechanism (exploited by PS) does not account for mismatches between the language that users employ in their queries and the ones used in tagged Freesound content. As this can impair the inclusion of non-english speakers, it motivated us to implement a translation tool, which now allows the user to select its own language and receive possible translations to English, the main language of the metadata. 

%Playsound's translation tool relies on Yandex APIs\footnote{\url{https://tech.yandex.com/translate/}}, that supports 90 languages. To simplify the interface, we reduced the range to 17 languages that are more present on Freesound. When users type in a keyword in the research field, PS issues a request to the translate API and propose the results to the users. They are then allowed to click on the suggestion to start the research with the English keyword.

%To illustrate how the translation tool can be useful for a user we provide an example based on the Portuguese word \textit{``pandeiro"} (i.e. tambourine in English). At the time of writing, Freesound hosts 52 sounds matching the word ``pandeiro". If Portuguese is set as the input language, PS translation let users query sounds with the English translation of the word, which yields 460 results.

\subsection{Tag and Audio Content-based Recommendation}

Another feature which we develop in parallel provides recommendations of audio files that sound similar to selected ones \cite{Viola2018}. This contribution is framed within the EU-funded Audio Commons project that aims at easing the access to Creative Commons audio content to the creative industries. The implemented recommendation mechanism is a complex multi-agent system based on the Semantic Web of Things. It operates through a two-stages approach: in the first stage it looks for audio files with the same tags on Freesound as well as on Europeana and Jamendo (all these three are content providers of the Audio Commons Ecosystem). In the second stage, an audio analysis is performed to reject results that differ too much from the originally selected file (at the moment a simple similarity function is implemented using spectral linear centroid computed with Sonic Annotator and Vamp plugins \cite{Cannam2010}).

\section{Discussion}

There are currently three instances of Playsound that are developed in parallel. The main instance can be used as a single user instrument or composition tool. The version including the chat system will be developed to become a fully participatory music making tool. The Playsound recommendation system should include in the future sounds from other resources and be integrated into the other instances upon positive result from testing. While some of the features developed came from necessities identified among the test users, other features followed design choices discussed by the authors to better support inclusion (for example the translation system) and to provide access to the Audio Commons Ecosystem (recommendations). Even though tests were made with different users during different phases of development, the core of the analysis mostly comes from the author's continuous practice with the system itself. Eventually, we realized that although porting the system to Web Audio buffers may offer more support in music processing, it also makes the system loose part of its real-time playing capabilities, as currently sound buffers need to be fully loaded before playing.







\begin{figure}[htb]
\label{audiqueryminipage}
\centering
 \begin{minipage}{0.49\textwidth}
   \centering
   \caption{Primeira versão da interface do software} \label{fig_minipage_imagem1}
   \includegraphics[width=\textwidth]{pictures/cap4/audioquery_firefox}
   \legend{Fonte: Screenshot da autora no navegador Chrome}
 \end{minipage}
 \hfill
 \begin{minipage}{0.49\textwidth}
   \centering
   \caption{} \label{fig_minipage_grafico2}
   \includegraphics[width=\textwidth]{pictures/cap4/audioquery_firefox}
   \legend{Fonte: Screenshot no navegador Firefox}
 \end{minipage}
\end{figure}



\subsection{Aplicação Prática}

\subsubsection{Curso de Edição, Captação e Produção de Audio Digital}

Voltando ao Brasil, assumi a vaga de professora efetiva na UFSB, para ministrar principalmente cursos para a habilitação em ``Arte e Produção Sonora'' do curso ``Som e Imagem em Movimento''. No primeiro componente que ministrei ``Edição, Captação e Produção de Áudio Digital'', que ministrei em conjunto com o professor Leonardo Souza, do curso de artes do corpo em cena.

Pudemos usar a ferramenta em diferentes contextos educacionais. Numa aula sobre timbre, por exemplo, reunimos num link vários exemplos de uma mesma nota gerada por instrumentos diferentes como diapasão, violino, clarinete, oboé, trompete, flauta, voz, piano, guitarra elétrica e diversos modelos diferentes de sintetizadores facilmente a partir do acervo do Freesound. Pudemos tocar todos esses sons facilmente e compará-los inclusive analisando e explicando os diferentes perfis de espectro de frequências sonoras.

Numa segunda oportunidade, o professor Leonardo trouxe uma série de playlists com diferentes tipos de sons como glissandos, batidas para realizar uma atividade de preparação de corpo sonoro com a turma de alunos. Um dos alunos operou com facilidade essas playlist improvisando com os sons reunidos enquanto o professor coordenava a atividade de corpo. 

Mais adiante no curso, quando partimos para produção musical, passamos a utilizar outras ferramentas como editores de áudio e trackers com sintetizadores e samplers. Nesta fase, usamos o Playsound principalmente para pesquisar sons para serem adicionados aos projetos dos alunos, que foram montados posteriormentes nos sequenciadores. Essa breve experiência foi importante para testar vários potenciais de aplicação do Playsound também na esfera educacional. Por ser livre, aberto e sem a necessidade de instalação, é uma ferramenta versátil que pode ser adotada por educadores em diversas práticas.


\subsection{Sarau do Binho}

Já no Brasil também, pude utilizar a ferramenta durante um evento organizado pela professora Cinara Araújo no Centro de Cultura da cidade de Porto Seguro. O evento tinha como proposta ser uma edição do ``Sarau do Binho'', que acontece regularmente na cidade de São Paulo, aproveitando a presença do poeta Binho, que estava de passagem pela Bahia. Durante o evento, utilizei a ferramenta para criar paisagens sonoras sobre as quais os participantes declamaram poemas de sua própria escolha ou autoria. A ferramenta se mostrou versátil para auxiliar a composição de diferentes ambientações sonoras que puderam acompanhar temas diversos dos oemas declamados no evento \footnote{Os sons utilizados na apresentaçnao podem ser acessados no endereço: \url{http://www.playsound.space/sounds=376415,419165,346105,320306,213318,4832,321030,125346,101195,52502,52499,84715,59356,166709,238689,396268}}.


\subsubsection{Cannibal Soundscapes}
Cannibal Soundscapes foi uma performance apresentada no Congresso UBIMUS de 2018. A proposta da performance era de produzir uma interpretação sonora do Manifesto Antropófago de Oswald de Andrade, \cite{Andrade1928}. Publicado em 1928 no primeiro número da ``Revista de Antropofagia'', o manifesto é considerado  marco teórico central do movimento antropofágico no Brasil.

O texto carrega referências diversas a teorias e autores, desde o pensamento revolucionário de Marx (1818- 1883), à idéia Freudiana de ``totem e tabu'', autores surrealistas como André Breton (1896 - 1966) e filósofos como Jean Jacques Rousseau (1712 - 1778), Francis Picabia (1879 - 1953) entre referências a figuras e movimentos da história brasileira. O manifesto traz a idéia de uma ``revolução Caraíba'', ``A unificação de todas as revoltas eficazes na direção do homem''. O antropófago é usado como uma metáfora para a devoração e digestão das influências culturais importadas, que deveriam ser repensadas criticamente sob os termos das condições locais \cite{Berg-1999}.

Para tanto, o manifesto se volta para a cultura indígena, nos lembrando que ``Já tínhamos o comunismo'' \cite{Bradley2007}, e se opondo a uma série de ``verdades'' trazidas junto com as caravelas que nos colonizaram. Propõe como horizonte utópico o ``matriarcado de Pindorama'', onde ``a alegria é a prova dos nove'':

\begin{citacao}
Contra a realidade social, vestida e opressora, cadastrada por Freud - a realidade sem complexos, sem loucura, sem prostituições e sem penitenciárias do matriarcado de Pindorama. \cite{Andrade1928}
\end{citacao}

\begin{figure}[hb!]
\centering
\includegraphics[width=1\linewidth]{pictures/cap4/diagrama-cannibal}
\caption{Diagrama para organizaçnao da performance no palco.}
\label{diagram}
\end{figure}

Por ser construído a partir de muitas referências da cultura brasileira, o Manifesto é um texto considerado difícil de traduzir \cite{Lesli-1991}. Para a performance, propusemos usar o texto original em português, e usar o sistema embutido de tradução como base para buscar as palavras no Freesound. Propusemos a performance como uma forma de aplicar também o sistema de chat que está sendo desenvolvido na plataforma. Nossa idéia era de simular um diálogo entre os dois performers, usando o texto de Oswald como base. Enquanto um dos performers (Mathieu Barthet) ia colando trechos do texto na tela, eu ia selecionando palavras e sons em tempo real, num processo de tradução intersemiótica \cite{JulioPlaza1969} mediada pelo sistema. 

A peça foi apresentada na sessão de concertos do Congresso UBIMUS, em São João Del Rey. A organização do evento teve dificuldades para conseguir internet de boa qualidade no local do evento, então o processamento pelo sistema foi mais lento do que esperávamos. Apesar de ter sido possível realizar a performance proposta, a instabilidade da rede causou um problema com o audio buffer, que a partir de um determinado momento, manteve um som em loop que não podia mais ser desligado.

A dificuldade em tocar sons longos no fez também rever a escolha da mudança de HTM5 para WebAudio, e para a próxima versão do software, queremos fazer uma versão mista, onde sons curtos sejam carregados no buffer e sons longos sejam tocados como objetos HTML.


\subsubsection{Tender Buttons | Sound Space}
Similar à performance anterior, onde usamos como base um texto para criar uma paisagem sonora, ``Tender Buttons | Sound | Space''  foi uma performance baseada no texto da Gertrude Stein ``Tender Buttons'' \cite{Stein1914}. Escrito em 1914, o poema traz combinações não usuais de palavras, que está associado com uma ideia de ``destruição da sintaxe'' \cite{Perloff1996}, que também subverte a fala e a prosa feminina tradicional \cite{Murphy1991}. O texto pode ser considerado como uma experiência de Stein com a liguagem, e é uma mistura de poesia e prosa com sentenças que a primeira vista podem parecer ``nonsense'', mas que ganham sentido à medida que se observa a forma que são empregadas \cite{Perloff1996}. Apesar de certas polêmicas a respeito da posição que a poeta veio defender na Segunda Guerra \cite{Bernstein2012}, que foi motivo de discussão entre a equipe, decidimos trabalhar com o poema por ser um dos poucos exemplos de prosa poética escrito por uma mulher livre de direitos autorais da Era moderna.

O texto é estruturado em três partes: \textit{Objects}, \textit{Food} e \textit{Rooms}. Nós escolhemos trabalhar com a terceira parte do poema, \textit{Rooms}, pela quantidade de sugestões sonoras que poderíamos usar durante a performance, como podemos ver neste trecho:

\begin{citacao}
Currents, currents are not in the air and on the floor and in the door and behind it first. Currents do not show it plainer. This which is mastered has so thin a space to build it all that there is plenty of room and yet is it quarreling, it is not and the insistence is marked. A change is in a current and there is no habitable exercise. \cite{Stein1914}
\end{citacao} 

A performance foi apresentada na Web Audio Conference de 2018, em Berlim, na Alemanha, em conjunto com Alessia Milo. Diferente da anterior, onde o texto era apresentado por escrito na tela mas a performance era apenas sonora, sem elementos discursivos, construímos essa performance baseada em uma leitura que eu fiz do poema, que foram cortados, selecionados e transferidos para o banco de dados do Freesound. Desta forma pudemos tocar esses trechos durante a performance e criar a paisagem sonora por cima das leituras. Assim, enquanto Alessia soltava os trechos da leitura, e colava o texto na janela do chat, eu ia selecionando os sons e tocando com eles. 

Pelas condições de som e internet no local, que eram excelentes, considero que essa performance foi mais bem sucedida que anterior, já que não houveram bugs e os sons foram carregados rápidamente. Em um momento, por volta da metade da performance, quando abri uma quantidade de sons considerável, senti que o sistema começou a ficar um pouco lento. Foi apenas remover alguns dos sons que já estavam carregados que a performance continou correndo normalmente. Um vídeo da performance, que durou 20 minutos está dispoível em: \url{https://www.youtube.com/watch?v=LiNb_T8oluA}

last
http://www.playsound.space/chat/sounds=316948,219902,88473,234539,421184,49044,235151,269384,303831,345836,435863,157487,233241,108443,238745,371820,184480,83538,83540,56306,197306,111113,263946

http://www.playsound.space/chat/sounds=156969,330136,330088,320305,197821,42222,211702,151215,384538,389165,398721,262705,269136,77207,399933,407258,197721,342563,346486,196491,196468,337655,88057,36807,126020,125408,373311,49044,235151,269384,303831,345836,435863,157487,233241,108443,238745,371820,184480,83538,83540,56306,197306,111113,373311,49044,235151,269384,303831,345836,435863,157487,233241,108443,218478,238749,238745,371820,184480,83538,83540,56306,197306,111113,436577,263946,410615,342214,375492,205551,311865,222651,220617,221550,343407,323731,334070,265328,405674,405672,221477,106658,210217,131264,213448,170779,277670,316948,219902,88473,234539,421184,68039,69439,269136,77207,399933,407258,196468,337655,88057,36807,126020,184480,342563,346486,126026,126027,126022,70070,70071,251462,167916,218478,184480,205551,390034,367302,367297,367299,343851,260860,32166,276717,151306,7991,335618,161667,210793,403072,332409,367302,367297,333613,72125,251743,212777,277643

This application uses several sounds from freesound.org

The sounds below are licensed with ``Attribution'' Creative Commons license and should be credited:

sound: Mini China 03.wav by: soundjoao; sound: Water short-1.wav by: SophronsineSoundDesign; sound: 300px-Spectrogram\_of\_violin3.wav by: NoiseCollector; sound: Cutting\_onion3.mp3 by: Taira Komori; sound: Water Swirl, Small, 22.wav by: InspectorJ; sound: Opening a heavy door (and bounding in) by: pfranzen; sound: Neutral Click Resonant Static Tube Sound.wav by: Heshl; sound: Sound efx (16).wav by: Spol; sound: GranulatorRiserPad.wav by: Sllarson; sound: Low (f).wav by: margo\_heston; sound: Aah (f).wav by: margo\_heston; sound: sherman current atmosphere06 rumble.wav by: schluppipuppie; sound: 20070628kijjaz-MovingACondensorMicStand02-48-24-mono.wav by: kijjaz; sound: I'm not speaking English by: krzysiunet; sound: Knife on Glass for a speech by: dersuperanton; sound: scissors.wav by: eduardfrigola; sound: door\_latch\_open.wav by: beerbelly38; sound: tearing paper\_09.wav by: Dymewiz; sound: DrawSword02.mp3 by: Yap\_Audio\_Production; sound: water\_bubbles\_02 by: audiolarx; sound: True 2.wav by: mohamedomarcomposer; sound: Weapon\_swing5.wav by: EverHeat; sound: Boys Pleasure Scream.mp3 by: pablocandel; sound: Knall17\_1zu8.wav by: Nikolino; sound: Question Mark.wav by: Augdog; sound: whycant.aiff by: bettsashl; sound: question.mp3 by: Taira Komori; sound: Surprise Breath1 by: theshaggyfreak; sound: GuyShock02.wav by: Otakua; sound: Water, Pouring, A.wav by: InspectorJ; sound: Messages whisper.wav by: arytopia; sound: gavel-single.flac by: zerolagtime; sound: gavel-triplemad.flac by: zerolagtime; sound: bass 0051 1-Audio-Bunt 1.wav by: reklamacja; sound: screech01.wav by: Pooleside; sound: Water dropping in a glass by: mboscolo; sound: carro 2.wav by: melack; sound: Sound efx (16).wav by: Spol; sound: Neutral Click Resonant Static Tube Sound.wav by: Heshl; sound: GranulatorRiserPad.wav by: Sllarson; sound: Aah (f).wav by: margo\_heston; sound: sherman current atmosphere06 rumble.wav by: schluppipuppie; sound: 20070628kijjaz-MovingACondensorMicStand02-48-24-mono.wav by: kijjaz; sound: I'm not speaking English by: krzysiunet; sound: Knife on Glass for a speech by: dersuperanton; sound: scissors.wav by: eduardfrigola; sound: Weapon\_swing5.wav by: EverHeat; sound: tearing paper\_09.wav by: Dymewiz; sound: DrawSword02.mp3 by: Yap\_Audio\_Production; sound: door\_latch\_open.wav by: beerbelly38;


\subsubsection{Imagina! Reverbera}

No meu segundo quadrimestre de trabalho na UFSB, ministrei o componente ``Oficina de prática em criação sonora'', onde trabalhamos processos de improvisação musical. Recebemos o convite da professora Juliana Gontijo para realizar uma sessão especial do projeto Imagina!, que realiza projeções de cinema em diversas sessões no município fazendo a sonorização ao vivo de filmes mudos. para a primeira sessão, selecionamos três filmes mudos para improvisar coletivamente sobre eles, criando uma trilha sonora ao vivo. A performance contou com a participação dos quatro alunos da turma: Gislania Araújo, Heictor Miranda Cruz, Herverton Taua Silva dos Santos e Marilucia Moreira, todos eles estreantes em performance ao vivo e iniciantes em práticas de improvisação musical, e dois outros alunos que já eram músicos, Eduardo Rebelo da Silva e Ítalo Rodrigues, que se juntaram pelo interesse no projeto. 

Na ocasião, eu e a aluna Gislânia usamos PS como ferramenta para tocar, em conjunto com instrumentos de percussão e voz. Os outros participantes tocaram com seus instrumentos tradicionais (computador, guitarra, bateria, voz, percussão). Como não sabíamos as condições de internet no local, decidimos trabalhar com sons pré-selecionados, como um roteiro para garantir que o sistema funcionasse mesmo sem internet, apesar de necessitar de internet para a busca de sons, o sistema funciona mesmo offline. Uma vez que os sons são carregados no buffer, é só manter a página aberta que podemos tocar com a ferramenta normalmente.

Para acompanhar os filmes, preparamos as seguintes playlists: 

\url{http://www.playsound.space/sounds=373811,36274,50737,360540,398712,238454,423526,397948,145685,47623,76420,76421,397948,417046,76422,435415,435414,340646,238456,238452,162761,264538,191240,274354}

\url{http://www.playsound.space/sounds=245381,394898,320303,24338,372181,411206,379249,266977,266916,382735,331624,321404,193900,188048,188051,373751,193810,193808,369913,433584,416439,7454,134968,101871,220910,326542,341561,411521,348519,378211,50820,50823}

Para tocar, utilizamos sons concretos, que tinham relação com as imagens apresentadas, como sons de mar, floresta, trânsito, bicicleta, e sons musicais para criar atmosferas de festa, ou de suspense dependendo do filme. Os sons tocados através da ferramenta funcionaram em grande parte como uma textura de base para os demais músicos improvisarem, garantindo um fluxo sonoro constante que ajudou a dar segurança ao demais participantes, que eram aprendizes na prática de improvisação livre.




\subsubsection{Transmusiking II}

Na semana do dia 20 de novembro de 2018, o grupo Female Laptop Orchestra, do qual também faço parte desde a conferência Audio Mostly de 2017, quando participei da performance da peça Transmusiking com o projeto Banda Aberta, participou de uma residência no Sonic Arts Research Institute (SARC) da Queen's University em Belfast. Devido ãs atividades de docência e pesquisa em andamento e à distância e os custos de transporte, pude participar apenas remotamente do processo, que contou com a participação das musicistas residentes Nela Brown (Londres, Inglaterra), Anna Xambo (Trondheim, Noruega), Magdalena Chudy (Warsaw, Polônia), Tuna Pase (Barcelona, Espanha), Liz Dobson (Huddersfield, UK), Ada Mathea Hoel (NTNU Norway) e Franziska Schroeder (Belfast, UK) e colaboração à distância de Sonia Wilkie (Melbourne, Austrália) e Lea Ikkache (Paris, França) além da minha, por streaming do Brasil. O projeto se propôs a realizar um painel sobre a participação das mulheres no campo da tecnologia musical ao redor do mundo e um concerto coletivo, para o qual propusemos uma nova versão da peça Transmusiking:


\begin{citacao}
Transmusicking II continues to explore geographical, cultural, technical and artistic challenges of collaborative music making, with co-located and distributed musicians who use multiple tools to relay musical information and create music together. This collaboration draws on the experience gained from Transmusicking I, premiered at Audio Mostly 2017, London.

As remote performers, despite real-time connectivity, we often experience a sense of loneliness. For this performance, we focus on the sense of “togetherness” by re-uniting musicians who come from distinct cultural backgrounds with different instruments and technologies. 

Distributed performers will use SARC’s mobile phone app liveSHOUT to send audio streams of cello, flute and live coding to the web-based Locus Sonus platform. Co-located musicians will improvise using saxophone, environmental loops and online sound libraries. 
The mix of the incoming streams and the onsite performers’ inputs will be spatialized into the performance space.

Live visuals will be produced to reinforce the themes of latency, collaboration, and togetherness. (email enviado por Magdalena Chudy à direção do SARC)
\end{citacao}

Durante a residência as participantes produziram uma partitura para organizar a participação das demais envolvidas no processo:

\begin{citacao}


SARC Concert

Each piece is approx. 10 minutes

To streamers: When you are streaming, don’t forget that you are solo during the first minute, so it will be good to play all your material and after the first minute you can take rests. 



1. Sonia streaming (Melbourne) 
1- 1 minute of  Sonia Streaming solo, Ada can take her stream and move here around the space. 
2-  Nela comes in with her 3D Belfast soundscape and Liz LFO. 
3-  We start duets as is we are cars passing by or people around. 
    1st duet Magda \& Ada- 2-3 minutes Ada 
    2nd duet Tuna (vox) \& Anna -2-3 minutes 
4- Sonia \& Nela with soundscapes.- Ada plays rhythmic and she stays solo at the end and fades out. 


2. Ariane streaming (São Paulo) 
1- 1 minute of  Ariane streaming solo, Ada can take her stream and move it around the space. (Ariane uses vocal, piano and soundscape of water)
2- Vive/Sax \& Liz duo - 5 minutes 
Ariane if you send piano play F,G,A
3- Voice duet  Ada \& Ariane- 2 minutes 
4- Play all breathy and watery  and Tuna (Wasserbass) fades out with water instrument - 1 minute   

3. Lea streaming (Paris) The drone piece and Tutti 
1- 1 minute of  Lea streaming solo, Ada can take her stream and move it around the space. 
2- 4 minute done build up (Tuna-Dronetext)
3- Flute que for the glitch/stacatto part to start and stop
Liz wobbles with the others. 
4- Stop and Rest!
5- Magda brings the drone back.
6- Fade out the drone until Lea’s singing bowl comes back. 
\end{citacao}

Para viabilizar a execução da peça, organizamos alguns ensaios via skype, onde testamos o setup no computador e o feedback recebido das performers que estavam em Belfast. Na minha participação no concerto, utilizei técnicas vocais expandidas para improvisar sobre uma base preparada no PS, onde selecionei sons de piano, de água e um sino invertido. 

Para fazer o \emph{streaming}, utilizei um aplicativo chamado Butt (Broadcast using this tool) que é um servidor alternativo de Icecast para windows e mac, utilizando o servidor \url{http://crowdj.net/}. Para enviar em conjunto o som que vinha do navegador e o som do microfone, foi preciso instalar um servidor de áudio que unia os fluxos sonoros de diferentes fontes do sistema. O atraso do som durante a transmissão era de cerca de sete segundos, e eu recebia o feedback das performance através do skype. Para não re-alimentar o som enviado com a resposta, o som era recebido da Irlanda pelo celular, enquanto eu tocava com o computador, o que me obrigou a usar dois fones de ouvido sobrepostos, para evitar vazamentos de áudio para o microfone. Apesar da logística complexa, a performance foi bem sucedida, devido a estratégias de tocar que não dependessem de um retorno imediato. Procurei produzir principalmente texturas, e por ser a fonte externa durante a parte da peça que toquei, acabava sendo um guia que enviava o som, enquanto as performers que estavam no local respondiam ao que eu enviava. 


Separei uma seleção de sons de piano: 


http://www.playsound.space/sounds=386440,351623,339821,316025,148571,176494,396483,427086,134908,332114,432441,232131,356107,373906,355826,355979,296145,148505,148508,409480,176494,351324,148548,295857,448552,148510,176504,351327,320306,320311,371494

sons usados:

\url{http://www.playsound.space/sounds=386440,351623,339821,316025,148571,176494,396483,427086,232131,296145,148505,148508,409480,176504,351327}

sound: Hand Bells, Reverse Cluster.wav by: InspectorJ; sound: PIANO\_MED\_F5.wav by: neatonk; sound: Piano key F3 by: Goup\_1; sound: PIANO\_LOUD\_F5.wav by: neatonk; sound: PIANO\_LOUD\_F6.wav by: neatonk; sound: Piano Key G\#6 by: Goup\_1;




\end{otherlanguage*}

\phantomsection



The Fall of the House of Usher  - 13 min - Melville Webber (1928)
\url{https://www.youtube.com/watch?v=mxjCWleWXf4}

Spook Sport - 8 min - Mary Ellen Bute (1940)
\url{http://www.ubu.com/film/bute_spook.html}

The Life and Death of 9413: A Hollywood Extra - 13  min - Slavko Vorkapich \& Robert Florey (1928)
\url{https://www.youtube.com/watch?v=b3M5znXDlm4}

The cameraman' s revenge  - 13 min - Ladislaw Starewicz (1912)
\url{https://www.youtube.com/watch?v=Q9lUcKtPTYY&feature=youtu.be}










%- amostra - limitação de sons para performances longas
%- audio mostly - feedback atrapalhou a performance